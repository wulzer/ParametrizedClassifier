{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OurTrainingTools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurModel(nn.Module):\n",
    "### Defines the  model with parametrized discriminant. Only quadratic dependence on a single parameter is implemented.\n",
    "### Input is the architecture (list of integers, the last one being equal to 1) and the activation type ('ReLU' or 'Sigmoid')\n",
    "    def __init__(self, AR = [1, 3, 3, 1] , AF = 'ReLU' ):               \n",
    "        super(OurModel, self).__init__() \n",
    "        ValidActivationFunctions = {'ReLU': torch.relu, 'Sigmoid': torch.sigmoid}\n",
    "        try:\n",
    "            self.ActivationFunction = ValidActivationFunctions[AF]\n",
    "        except KeyError:\n",
    "            print('The activation function specified is not valid. Allowed activations are %s.'\n",
    "                 %str(list(ValidActivationFunctions.keys())))\n",
    "            print('Will use ReLU.')\n",
    "            self.ActivationFunction = torch.relu            \n",
    "        if type(AR) == list:\n",
    "            if( ( all(isinstance(n, int) for n in AR)) and ( AR[-1] == 1) ):\n",
    "                self.Architecture = AR\n",
    "            else:\n",
    "                print('Architecture should be a list of integers, the last one should be 1.')\n",
    "                raise ValueError             \n",
    "        else:\n",
    "            print('Architecture should be a list !')\n",
    "            raise ValueError\n",
    "\n",
    "### Define Layers\n",
    "        self.LinearLayerList1  = nn.ModuleList([nn.Linear(self.Architecture[i], \n",
    "            self.Architecture[i+1]) for i in range(len(self.Architecture)-2)])\n",
    "        self.OutputLayer1 = nn.Linear(self.Architecture[-2], 1)       \n",
    "        self.LinearLayerList2 = nn.ModuleList([nn.Linear(self.Architecture[i], \n",
    "            self.Architecture[i+1]) for i in range(len(self.Architecture)-2)])\n",
    "        self.OutputLayer2 = nn.Linear(self.Architecture[-2], 1)\n",
    "        \n",
    "        #self.Optimiser = torch.optim.Adam(self.parameters(), self.InitialLearningRate)\n",
    "        #self.Criterion = WeightedMSELoss()\n",
    "\n",
    "    def Forward(self, Data, Parameters):\n",
    "### Forward Function. Performs Preprocessing, returns F = rho/(1+rho) in [0,1], where rho is quadratically parametrized.\n",
    "        # Checking that data has the right input dimension\n",
    "        InputDimension = self.Architecture[0]\n",
    "        if Data.size(1) != InputDimension:\n",
    "            print('Dimensions of the data and the network input mismatch: data: %d, model: %d'\n",
    "                  %(Data.size(1), InputDimension))\n",
    "            raise ValueError\n",
    "\n",
    "        # Checking that preprocess has been initialised\n",
    "        if not hasattr(self, 'Shift'):\n",
    "            print('Please initialize preprocess parameters!')\n",
    "            raise ValueError\n",
    "        with torch.no_grad(): \n",
    "            Data, Parameters = self.Preprocess(Data, Parameters)  \n",
    "        \n",
    "        x1 = x2 = Data\n",
    "        \n",
    "        for i, Layer in enumerate(self.LinearLayerList1):\n",
    "            x1 = self.ActivationFunction(Layer(x1)) \n",
    "        x1 = self.OutputLayer1(x1).squeeze()\n",
    "        \n",
    "        for i, Layer in enumerate(self.LinearLayerList2):\n",
    "            x2 = self.ActivationFunction(Layer(x2))\n",
    "        x2 = self.OutputLayer2(x2).squeeze()\n",
    "        \n",
    "        rho = (1 + torch.mul(x1, Parameters))**2 + (torch.mul(x2, Parameters))**2  \n",
    "        return (rho.div(1.+rho)).view(-1, 1)\n",
    "    \n",
    "    def GetL1Bound(self, L1perUnit):\n",
    "### Max L1 norm of weights at each layer. What about bias? Excluding the first layer?\n",
    "        self.L1perUnit = L1perUnit\n",
    "        L1MaxList = []\n",
    "        for m in self.children():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                L1MaxList.append(m.weight.size(0)*m.weight.size(1) \\\n",
    "                                  *self.L1perUnit)\n",
    "            else:\n",
    "                for mm in m:\n",
    "                    L1MaxList.append(mm.weight.size(0)*mm.weight.size(1)\\\n",
    "                                      *self.L1perUnit)\n",
    "        self.L1MaxList = L1MaxList\n",
    "        print('L1MaxList created.')\n",
    "    \n",
    "    def ClipL1Norm(self):\n",
    "### Clip the weights      \n",
    "        def ClipL1NormLayer(DesignatedL1Max, Layer):\n",
    "            L1 = Layer.weight.abs().sum()\n",
    "            Layer.weight.masked_scatter_(L1 > DesignatedL1Max, \n",
    "                                        Layer.weight*(DesignatedL1Max/L1))\n",
    "        \n",
    "        Counter = 0\n",
    "        for m in self.children():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                Counter += 1\n",
    "                with torch.no_grad():\n",
    "                    DesignatedL1Max = self.L1MaxList[counter-1]\n",
    "                    if Counter != 1: ClipL1NormLayer(DesignatedL1Max, m)\n",
    "                    ### this avoids clipping the first layer\n",
    "            else:\n",
    "                for mm in m:\n",
    "                    Counter +=1\n",
    "                    with torch.no_grad():\n",
    "                        DesignatedL1Max = self.L1MaxList[counter-1]\n",
    "                        if Counter != 1: ClipL1NormLayer(DesignatedL1Max, mm)\n",
    "                        ### this avoids clipping the first layer\n",
    "        return \n",
    "    \n",
    "    def DistributionRatio(self, points):\n",
    "### This is rho. I.e., after training, the estimator of the distribution ratio.\n",
    "        with torch.no_grad():\n",
    "            F = self(points)\n",
    "        return F/(1-F)\n",
    "\n",
    "    def InitPreprocess(self, Data, Parameters):\n",
    "### This can be run only ONCE to initialize the preprocess (shift and scaling) parameters\n",
    "### Takes as input the training Data and the training Parameters as Torch tensors.\n",
    "        if not hasattr(self, 'Scaling'):\n",
    "            print('Initializing Preprocesses Variables')\n",
    "            self.Scaling = Data.std(0)\n",
    "            self.Shift = Data.mean(0)\n",
    "            self.ParameterScaling = Parameters.std(0)  \n",
    "        else: print('Preprocess can be initialized only once. Parameters unchanged.')\n",
    "            \n",
    "    def Preprocess(self, Data, Parameters):\n",
    "### Returns scaled/shifted data and parameters\n",
    "### Takes as input Data and Parameters as Torch tensors.\n",
    "        if  not hasattr(self, 'Scaling'): print('Preprocess parameters are not initialized.')\n",
    "        Data = (Data - self.Shift)/self.Scaling\n",
    "        Parameters = Parameters/self.ParameterScaling\n",
    "        return Data, Parameters\n",
    "    \n",
    "    def Save(self, Name, Folder):\n",
    "### Saves the model in Folder/Name\n",
    "        FileName = Folder + Name + '.pth'\n",
    "        torch.save({'StateDict': self.state_dict(), \n",
    "                   'Scaling': self.Scaling,\n",
    "                   'Shift': self.Shift,\n",
    "                   'ParameterScaling': self.ParameterScaling}, \n",
    "                   FileName)\n",
    "        print('Model successfully saved.')\n",
    "        print('Path: %s'%str(FileName))\n",
    "    \n",
    "    def Load(self, Name, Folder):\n",
    "### Loads the model from Folder/Name\n",
    "        FileName = Folder + Name + '.pth'\n",
    "        try:\n",
    "            IncompatibleKeys = self.load_state_dict(torch.load(FileName)['StateDict'])\n",
    "        except KeyError:\n",
    "            print('No state dictionary saved. Loading model failed.')\n",
    "            return \n",
    "        \n",
    "        if list(IncompatibleKeys)[0]:\n",
    "            print('Missing Keys: %s'%str(list(IncompatibleKeys)[0]))\n",
    "            print('Loading model failed. ')\n",
    "            return \n",
    "        \n",
    "        if list(IncompatibleKeys)[1]:\n",
    "            print('Unexpected Keys: %s'%str(list(IncompatibleKeys)[0]))\n",
    "            print('Loading model failed. ')\n",
    "            return \n",
    "        \n",
    "        self.Scaling = torch.load(ModelPath + Name + '.pth')['Scaling']\n",
    "        self.Shift = torch.load(ModelPath + Name + '.pth')['Shift']\n",
    "        self.ParameterScaling = torch.load(ModelPath + Name + '.pth')['ParameterScaling']\n",
    "        \n",
    "        print('Model successfully loaded.')\n",
    "        print('Path: %s'%str(FileName))\n",
    "        \n",
    "    def Report(self): ### is it possibe to check if the model is in double?\n",
    "        print('\\nModel Report:')\n",
    "        print('Preprocess Initialized: ' + str(hasattr(self, 'Shift')))\n",
    "        print('Architecture: ' + str(self.Architecture))\n",
    "        print('Loss Function: ' + 'Quadratic')\n",
    "        print('Activation: ' + str(self.ActivationFunction))\n",
    "        \n",
    "    def cuda(self):\n",
    "        nn.Module.cuda(self)\n",
    "        self.Shift = self.Shift.cuda()\n",
    "        self.Scaling = self.Scaling.cuda()\n",
    "        self.ParameterScaling = self.ParameterScaling.cuda()\n",
    "        \n",
    "    def cpu(self):\n",
    "        self.Shift = self.Shift.cpu()\n",
    "        self.Scaling = self.Scaling.cpu()\n",
    "        self.ParameterScaling = self.ParameterScaling.cpu()\n",
    "        return nn.Module.cpu(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del TD\n",
    "TD = OurTrainingData(['/data3/Training/TrainingData/SMData/ChPsm2.h5'], \n",
    "                     ['/data3/Training/TrainingData/GWData/ChPgw2.h5', '/data3/Training/TrainingData/GWData/ChPgwm2.h5',\n",
    "                      '/data3/Training/TrainingData/GWData/ChPgw5.h5'],\n",
    "                     parameters = ['GW[TeV**-2]'], process = 'W+Z', \n",
    "                     verbose=False)# , BSMNLimits = [ 300000, 300000, 300000 ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TD = OurTrainingData(['/data3/Training/TrainingData/GphiData/ChPsm.h5'], \n",
    " #                    ['/data3/Training/TrainingData/GphiData/ChPgphi50.h5','/data3/Training/TrainingData/GphiData/ChPgphim50.h5'],\n",
    "  #                   parameters = ['Gphi[TeV**-2]'], process = 'W+Z', \n",
    "   #                  verbose=False)# , BSMNLimits = [ 300000, 300000, 300000 ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def OurCudaTensor(input):\n",
    "    output = copy.deepcopy(input)\n",
    "    output = output.cuda()\n",
    "    return output\n",
    "\n",
    "class OurTrainer(nn.Module):\n",
    "### Contains all parameters for training: Loss Function, Optimiser, NumberOfEpochs, InitialLearningRate, SaveAfterEpoch \n",
    "    def __init__(self, LearningRate = 1e-3, LossFunction = 'Quadratic', Optimiser = 'Adam', NumEpochs = 100):\n",
    "        super(OurTrainer, self).__init__() \n",
    "        self.NumberOfEpochs = NumEpochs\n",
    "        self.InitialLearningRate = LearningRate\n",
    "        ValidCriteria = {'Quadratic': WeightedMSELoss()}\n",
    "        try:\n",
    "            self.Criterion = ValidCriteria[LossFunction]\n",
    "        except KeyError:\n",
    "            print('The loss function specified is not valid. Allowed losses are %s.'\n",
    "                 %str(list(ValidCriteria)))\n",
    "            print('Will use Quadratic Loss.') \n",
    "        ValidOptimizers = {'Adam': torch.optim.Adam}\n",
    "        try:\n",
    "            self.Optimiser =  ValidOptimizers[Optimiser]\n",
    "        except KeyError:\n",
    "            print('The specified optimiser is not valid. Allowed optimisers are %s.'\n",
    "                 %str(list(ValidOptimisers)))\n",
    "            print('Will use Adam.')          \n",
    "    \n",
    "    def EstimateRequiredGPUMemory(self, model, Data, Parameters):\n",
    "        if next(model.parameters()).is_cuda:\n",
    "            print('Model is on cuda. No estimate possible anymore.')\n",
    "            return None\n",
    "        else:\n",
    "            before = torch.cuda.memory_allocated()\n",
    "            print(before)\n",
    "            ### Always make deep copy of objects before sending them to cuda. Delete when done\n",
    "            ModelCuda = copy.deepcopy(model)\n",
    "            ModelCuda.cuda()\n",
    "            DataCuda = OurCudaTensor(Data[:10000])\n",
    "            ParametersCuda = OurCudaTensor(Parameters[:10000])\n",
    "            print(torch.cuda.memory_allocated())\n",
    "            MF = ModelCuda.Forward(DataCuda, ParametersCuda)\n",
    "            after = torch.cuda.memory_allocated()\n",
    "            print(after)\n",
    "            del ModelCuda, DataCuda, ParametersCuda, MF\n",
    "            torch.cuda.empty_cache()        \n",
    "            estimate = float(Data.size()[0])/1e4*float(after-before)*1e-9\n",
    "            print(str(estimate) + ' GB')\n",
    "            return estimate\n",
    "        \n",
    "    def Train(self, model, Data, Parameters, Labels, Weights, L1perUnit=None, UseGPU=True, Name=\"\", Folder=os.getcwd()):\n",
    "        \n",
    "        tempmodel = copy.deepcopy(model)\n",
    "        tempmodel.cuda()\n",
    "        tempData = OurCudaTensor(Data)\n",
    "        tempParameters = OurCudaTensor(Parameters)\n",
    "        tempLabels = OurCudaTensor(Labels)\n",
    "        tempWeights = OurCudaTensor(Weights)\n",
    "        \n",
    "        self.Optimiser = self.Optimiser(tempmodel.parameters(), self.InitialLearningRate)\n",
    "        mini_batch_size = 100\n",
    "        for e in range(self.NumberOfEpochs):\n",
    "            print(\"epoch\")\n",
    "            self.Optimiser.zero_grad()\n",
    "            for b in range(0, Data.size(0), mini_batch_size):\n",
    "                torch.cuda.empty_cache()\n",
    "                output          = tempmodel.Forward(tempData[b:b+mini_batch_size], tempParameters[b:b+mini_batch_size])\n",
    "                loss            = self.Criterion(output, tempLabels[b:b+mini_batch_size].reshape(-1,1), \n",
    "                                                 tempWeights[b:b+mini_batch_size].reshape(-1, 1))               \n",
    "                loss.backward()\n",
    "            self.Optimiser.step()\n",
    "        return tempmodel.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#del OT\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2309217"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TD.ParVal.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Preprocesses Variables\n",
      "tensor([[0.4892]], dtype=torch.float64, grad_fn=<ViewBackward>)\n",
      "epoch\n",
      "epoch\n",
      "epoch\n",
      "epoch\n"
     ]
    }
   ],
   "source": [
    "MD = OurModel(AR=[11,32,32,32,1])\n",
    "MD.double()\n",
    "MD.InitPreprocess(TD.Data, TD.ParVal)\n",
    "print(MD.Forward(TD.Data[:1],TD.ParVal[:1]))\n",
    "OT = OurTrainer(NumEpochs = 100)\n",
    "MD = OT.Train(MD,Data = TD.Data, Parameters = TD.ParVal, Labels=TD.Labels, Weights=TD.Weights)\n",
    "print(MD.Forward(TD.Data[:1],TD.ParVal[:1]))\n",
    "print(torch.cuda.memory_allocated())\n",
    "del OT\n",
    "torch.cuda.empty_cache()\n",
    "print(MD.Forward(TD.Data[:1],TD.ParVal[:1]))\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 32, 32, 32, 1]\n",
      "Initializing Preprocesses Variables\n",
      "0\n",
      "1006080\n",
      "33580032\n",
      "7.754358075494401 GB\n"
     ]
    }
   ],
   "source": [
    "#del MD\n",
    "MD = OurModel(AR=[11,32,32,32,1])\n",
    "print(MD.Architecture)\n",
    "MD.double()\n",
    "MD.InitPreprocess(TD.Data, TD.ParVal)\n",
    "OT = OurTrainer()\n",
    "OT.EstimateRequiredGPUMemory(MD, TD.Data, TD.ParVal)\n",
    "del MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 200, 200, 1]\n",
      "Initializing Preprocesses Variables\n",
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 764.00 MiB (GPU 0; 7.92 GiB total capacity; 5.31 GiB already allocated; 764.25 MiB free; 5.33 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-2b1586743817>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mMD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInitPreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParVal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mOT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOurTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mOT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEstimateRequiredMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParVal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mMD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-cf550319bab3>\u001b[0m in \u001b[0;36mEstimateRequiredMemory\u001b[0;34m(self, Data, Parameters)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mParametersCuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParametersCuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mafter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_allocated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mModelCuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataCuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParametersCuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mModelCuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataCuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParametersCuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c11bcb29cfdc>\u001b[0m in \u001b[0;36mForward\u001b[0;34m(self, Data, Parameters)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearLayerList2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActivationFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutputLayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 764.00 MiB (GPU 0; 7.92 GiB total capacity; 5.31 GiB already allocated; 764.25 MiB free; 5.33 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "#del MD\n",
    "MD = OurModel(AR=[11,200,200,1])\n",
    "print(MD.Architecture)\n",
    "MD.double()\n",
    "MD.InitPreprocess(TD.Data, TD.ParVal)\n",
    "OT = OurTrainer(MD)\n",
    "OT.EstimateRequiredMemory(TD.Data, TD.ParVal)\n",
    "del MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 200, 200, 200, 1]\n",
      "Initializing Preprocesses Variables\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 3.44 GiB (GPU 0; 7.92 GiB total capacity; 3.86 GiB already allocated; 2.04 GiB free; 4.03 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3085220f3fd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mMD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInitPreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParVal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mMD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mMD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParVal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-c11bcb29cfdc>\u001b[0m in \u001b[0;36mForward\u001b[0;34m(self, Data, Parameters)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearLayerList1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActivationFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutputLayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.44 GiB (GPU 0; 7.92 GiB total capacity; 3.86 GiB already allocated; 2.04 GiB free; 4.03 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "MD = OurModel(AR=[11,200,200,200,1])\n",
    "print(MD.Architecture)\n",
    "MD.double()\n",
    "MD.InitPreprocess(TD.Data, TD.ParVal)\n",
    "MD.cuda()\n",
    "MD.Forward(TD.Data.cuda(), TD.ParVal.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#del OT\n",
    "#torch.cuda.empty_cache() \n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(OT.Model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.element_size()*Data.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TD.Data[:1000].size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TD.Data.cuda()\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885760\n",
      "8805888\n"
     ]
    }
   ],
   "source": [
    "dd = TD.Data[:10000].cuda()\n",
    "print(torch.cuda.memory_allocated())\n",
    "del dd\n",
    "\n",
    "dd = TD.Data[:100000].cuda()\n",
    "print(torch.cuda.memory_allocated())\n",
    "del dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#del MD\n",
    "#del OT\n",
    "#torch.cuda.empty_cache()\n",
    "#LD=MD\n",
    "#LD.cuda()\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b19e022b6d6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'MD' is not defined"
     ]
    }
   ],
   "source": [
    "next(MD.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0288, dtype=torch.float64)\n",
      "0\n",
      "5632\n",
      "-5632\n",
      "1.3113126400000001GB\n"
     ]
    }
   ],
   "source": [
    "TR = OurTrainer(MD)\n",
    "print(MD.ParameterScaling)\n",
    "TR.EstimateRequiredMemory(TD.Data, TD.ParVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1486086"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TD.Data.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11000, 11])\n",
      "torch.float64\n",
      "[torch.Size([10000]), torch.Size([1000]), torch.Size([2000])]\n",
      "[torch.float64, torch.float64, torch.float64]\n",
      "\n",
      "Loaded SM Files:\n",
      "  ['GW[TeV**-2]']    #Data    XS[pb](avg.w)\n",
      "-----------------  -------  ---------------\n",
      "                0   566980      0.000322493\n",
      "                0    19108      0.000325419\n",
      "\n",
      "Loaded BSM Files:\n",
      "  ['GW[TeV**-2]']    #Data    XS[pb](avg.w)\n",
      "-----------------  -------  ---------------\n",
      "             0.02   571169      0.000324063\n",
      "            -0.02   573538      0.000329051\n",
      "             0.05   597532      0.000340505\n",
      "\n",
      "Paired BSM/SM Datasets:\n",
      "\n",
      "  ['GW[TeV**-2]']    #Ev.BSM    #Ev.SM    sum.w SM\\/XSSM\n",
      "-----------------  ---------  --------  ----------------\n",
      "             0.02      10000      8461          9979.43\n",
      "            -0.02       1000       846           974.481\n",
      "             0.05       2000      1692          2045.99\n"
     ]
    }
   ],
   "source": [
    "TD = OurTrainingData(['/data3/Training/TrainingData/GWData/ChPsm.h5', '/data3/MadGraph/testSM1.h5'], \n",
    "                     ['/data3/Training/TrainingData/GWData/ChPgw2.h5', '/data3/Training/TrainingData/GWData/ChPgwm2.h5',\n",
    "                      '/data3/Training/TrainingData/GWData/ChPgw5.h5'],\n",
    "                    ['GW[TeV**-2]'], verbose=False, SMNLimits = [ 10000, 1000 ], BSMNLimits = [ 10000, 1000, 2000 ] )\n",
    "print(TD.SMData.size())\n",
    "print(TD.SMData.dtype)\n",
    "print([w.size() for w in TD.BSMWeightsList])\n",
    "print([w.dtype for w in TD.BSMWeightsList])\n",
    "TD.Report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Init test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py, torch, time, datetime, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn.modules import Module\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFile():\n",
    "### Reads sample file Info (string), Parameters (list), Values (torch array), Data (torch array) and Weights (torch array)\n",
    "### FilePath is the path of the input file\n",
    "### Computes cross-section XS (average weight) and total number of data ND in file\n",
    "### Checks that files are in correct format (correct Keys)\n",
    "### and that the length of Parameters and Data equals the one of Values and Weights respectively\n",
    "    def __init__(self, FilePath, verbose=True):\n",
    "        print('\\nReading file ...' + FilePath)\n",
    "        file = h5py.File(FilePath, 'r')\n",
    "        if list(file.keys()) == ['Data', 'Info', 'Parameters', 'Values', 'Weights']:\n",
    "            if( (len(file['Parameters'][()]) == len(file['Values'][()])) and (len(file['Data'][()]) == len(file['Weights'][()])) ):\n",
    "                if verbose: print('##### File Info:\\n' + file['Info'][()][0] + '\\n#####')\n",
    "                self.FilePath = FilePath\n",
    "                self.Info = file['Info'][()][0]\n",
    "                self.Parameters = file['Parameters'][()]\n",
    "                #print(file['Values'][()].dtype)\n",
    "                #print('%.15f' % (file['Data'][()][0][0]))\n",
    "                self.Values = torch.DoubleTensor(file['Values'][()])\n",
    "                self.Data = torch.DoubleTensor(file['Data'][()])\n",
    "                self.Weights = torch.DoubleTensor(file['Weights'][()])\n",
    "                self.XS = self.Weights.mean()\n",
    "                self.ND = len(self.Weights)\n",
    "            else: print('--> File not valid:\\nunequal lenght of Values and Parameters or of Data and Weights')\n",
    "        else:\n",
    "            print('--> File format not valid:\\nKeys: ' + print(list(file.keys())) + \n",
    "                  'should be ' + print(['Data', 'Info', 'Parameters', 'Values', 'Weights']))\n",
    "            \n",
    "#test = []\n",
    "#test.append(DataFile('/data3/Training/TrainingData/GWData/ChPsm.h5'))\n",
    "#test.append(DataFile('/data3/Training/TrainingData/GWData/ChPgw10.h5'))\n",
    "#test.append(DataFile('/data3/MadGraph/testSM1.h5'))\n",
    "\n",
    "test = [ DataFile(path) for path in ['/data3/Training/TrainingData/GWData/ChPsm.h5', '/data3/MadGraph/testSM1.h5', '/data3/MadGraph/testSM1.h5'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test[2].Values.tolist())\n",
    "print(test[2].FilePath)\n",
    "print(test[2].Values.dtype)\n",
    "print(test[2].Weights.dtype)\n",
    "print(test[2].Data.dtype)\n",
    "print(test[2].XS.dtype)\n",
    "print(type(test[2].ND))\n",
    "print('%.15f' % test[2].Weights[0])\n",
    "print('%.15f' % test[2].Data[0][2])\n",
    "print('%.15f' % test[2].XS)\n",
    "print('%.15f' % test[0].XS)\n",
    "print((1.0-test[2].XS/test[0].XS).dtype)\n",
    "print('%.15f' % (1.0-test[2].XS/test[0].XS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test[0].Values)\n",
    "print(test[1].Values)\n",
    "print(test[1].XS)\n",
    "print(test[1].ND)\n",
    "print(type(test[0].ND))\n",
    "len(test[1].Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = SampleFile('/data3/Training/TrainingData/GWData/ChMsm.h5')\n",
    "print(test.Values.dtype)\n",
    "print(test.Data.dtype)\n",
    "print(test.Weights.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.DoubleTensor([2.,3.4])\n",
    "print(a)\n",
    "a = a*4./8.\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurTrainingData():\n",
    "    def __init__(self, SMfilepathlist, BSMfilepathlist, parameters, SMNLimits=\"NA\", BSMNLimits=\"NA\", verbose=True): \n",
    "        self.Parameters = parameters\n",
    "        print('Loading Data Files with Parameters: ' + str(self.Parameters) ) \n",
    "        if len(self.Parameters)!= 1: print('Only 1D Implemented in Training !')   \n",
    "  \n",
    "        \n",
    "####### Load BSM data (stored in self.BSMDataFiles)\n",
    "        if type(BSMfilepathlist) == list:\n",
    "            if all(isinstance(n, str) for n in BSMfilepathlist):\n",
    "                #self.BSMFilePathList = BSMfilepathlist\n",
    "                #self.BSMNumFiles = len(self.BSMFilePathList)\n",
    "                self.BSMDataFiles = []\n",
    "                for path in BSMfilepathlist:\n",
    "                    temp =  DataFile(path, verbose=verbose)\n",
    "                    if( (temp.Parameters == self.Parameters) and (temp.Values != 0.) ):\n",
    "                        self.BSMDataFiles.append(temp)\n",
    "                    else: \n",
    "                        print('File not valid: ' + path)\n",
    "                        print('Parameters = ' + str(temp.Parameters) + ' and Values = ' + str(temp.Values.tolist()))\n",
    "                        print('should be = ' + str(self.Parameters) + ' and != ' + str(0.))\n",
    "                        raise ValueError\n",
    "                        self.BSMDataFiles.append(None) \n",
    "            else:\n",
    "                print('BSMfilepathlist input should be a list of strings !')\n",
    "                raise FileNotFoundError\n",
    "        else:\n",
    "            print('BSMfilepathlist input should be a list !')\n",
    "            raise FileNotFoundError\n",
    "                  \n",
    "###### Chop the BSM data sets (stored in BSMNDList, BSMDataList, BSMWeightsList, BSMParValList, BSMTargetList)\n",
    "        if type(BSMNLimits) == int:\n",
    "            BSMNLimits = [BSMNLimits for data in self.BSMDataFiles]\n",
    "        elif type(BSMNLimits) == list and all(isinstance(n, int) for n in BSMNLimits):\n",
    "            if len(BSMNLimits) != len(self.BSMDataFiles):\n",
    "                print(\"--> Please input %d integers to chop each SM file.\"%(\n",
    "                    len(self.BSMDataFiles)))\n",
    "                raise ValueError\n",
    "            elif sum([self.BSMDataFiles[i].ND >= BSMNLimits[i] for i in range(len(BSMNLimits))]\n",
    "                    ) != len(self.BSMDataFiles):\n",
    "                print(\"--> Some chop limit larger than available data in the corresponding file.\")\n",
    "                print(\"--> Lengths of the files: \"+str([file.ND for file in self.BSMDataFiles ]))\n",
    "                raise ValueError\n",
    "        else:\n",
    "            BSMNLimits =[file.ND for file in self.BSMDataFiles]   \n",
    "            \n",
    "        self.BSMNDList = BSMNLimits\n",
    "        #self.BSMNData = sum(self.BSMNDataList)\n",
    "        self.BSMDataList = [DF.Data[:N] for (DF, N) in zip(\n",
    "            self.BSMDataFiles, self.BSMNDList)]\n",
    "        self.BSMWeightsList = [DF.Weights[:N] for (DF, N) in zip(\n",
    "            self.BSMDataFiles, self.BSMNDList)] \n",
    "        self.BSMValuesList = [torch.ones(N)*DF.Values for (DF, N) in zip(\n",
    "            self.BSMDataFiles, self.BSMNDList)]\n",
    "        \n",
    "        self.BSMParValList =  [torch.ones(N, )*DF.Values for (DF, N) in zip(self.BSMDataFiles, self.BSMNDList)]\n",
    "        \n",
    "        self.BSMTargetList = [torch.ones(N, dtype=torch.double) for N in self.BSMNDList] \n",
    "        \n",
    "####### Load SM data (stored in SMDataFiles)\n",
    "        if type(SMfilepathlist) == list:\n",
    "            if all(isinstance(n, str) for n in SMfilepathlist):\n",
    "                #self.SMFilePathList = SMfilepathlist\n",
    "                #self.SMNumFiles = len(self.SMFilePathList)\n",
    "                self.SMDataFiles = []\n",
    "                for path in SMfilepathlist:\n",
    "                    temp =  DataFile(path, verbose=verbose)\n",
    "                    if( (temp.Parameters == self.Parameters) and (temp.Values == 0.) ):\n",
    "                       self.SMDataFiles.append(temp)\n",
    "                    else:\n",
    "                       print('File not valid: ' + path)\n",
    "                       print('Parameters = ' + str(temp.Parameters) + ' and Values = ' + str(temp.Values.tolist()))\n",
    "                       print('should be = ' + str(self.Parameters) + ' and = ' + str(0.))\n",
    "                       self.SMDataFiles.append(None)                    \n",
    "            else:\n",
    "                print('SMfilepathlist input should be a list of strings !')\n",
    "                raise FileNotFoundError\n",
    "        else:\n",
    "            print('SMfilepathlist input should be a list !')\n",
    "            raise FileNotFoundError\n",
    "            \n",
    "####### Chop the SM data sets and join them in one (stored in SMND, SMData and SMWeights)\n",
    "        if type(SMNLimits) == int:\n",
    "            SMNLimits = [SMNLimits for data in self.SMDataFiles]\n",
    "        elif type(SMNLimits) == list and all(isinstance(n, int) for n in SMNLimits):\n",
    "            if len(SMNLimits) != len(self.SMDataFiles):\n",
    "                print(\"--> Please input %d integers to chop each SM file.\"%(\n",
    "                    len(self.SMDataFiles)))\n",
    "                raise ValueError\n",
    "            elif sum([self.SMDataFiles[i].ND >= SMNLimits[i] for i in range(len(SMNLimits))]\n",
    "                    ) != len(self.SMDataFiles):\n",
    "                print(\"--> Some chop limit larger than available data in the corresponding file.\")\n",
    "                print(\"--> Lengths of the files: \" + str([file.ND for file in self.SMDataFiles]))\n",
    "                raise ValueError\n",
    "        else:\n",
    "            SMNLimits = [file.ND for file in self.SMDataFiles]\n",
    "        self.SMND = sum(SMNLimits)\n",
    "        self.SMData = torch.cat(\n",
    "            [DF.Data[:N] for (DF, N) in zip(self.SMDataFiles, SMNLimits)]\n",
    "            , 0) \n",
    "        self.SMWeights = torch.cat(\n",
    "            [DF.Weights[:N] for (DF, N) in zip(self.SMDataFiles, SMNLimits)]\n",
    "            , 0)\n",
    "\n",
    "####### Break SM data in blocks to be paired with BSM data (stored in UsedSMNDList, UsedSMDataList, UsedSMWeightsList, UsedSMParValList, UsedSMTargetList)\n",
    "        BSMNRatioDataList = [torch.tensor(1., dtype=torch.double)*n/sum(self.BSMNDList\n",
    "                                                                       ) for n in self.BSMNDList]\n",
    "        self.UsedSMNDList = [int(self.SMND*BSMNRatioData) for BSMNRatioData in BSMNRatioDataList] \n",
    "        #self.UsedSMNData = sum(self.UsedSMNDataList)\n",
    "        #self.UsedSMData = self.SMData[: self.UsedSMND]\n",
    "        self.UsedSMDataList =  self.SMData[:sum(self.UsedSMNDList)].split(self.UsedSMNDList)\n",
    "        \n",
    "    ##### Reweighting is performed such that the SUM of the SM weights in each block equals the number of BSM data times the AVERAGE \n",
    "    ##### of the original weights. This equals the SM cross-section as obtained in the specific sample at hand, times NBSM\n",
    "        self.UsedSMWeightsList = self.SMWeights[:sum(self.UsedSMNDList)].split(self.UsedSMNDList)\n",
    "        self.UsedSMWeightsList = [ self.UsedSMWeightsList[i]*self.BSMNDList[i]/self.UsedSMNDList[i] for i in range(len(BSMNRatioDataList))]\n",
    "        \n",
    "        #ReWeighting = torch.cat([torch.ones(self.UsedSMNDList[i], dtype=torch.double).mul(self.BSMNDList[i]\n",
    "                                          #  ).div(self.UsedSMNDList[i]) for i in range(len(BSMNRatioDataList))])\n",
    "        #self.UsedSMWeightsList = self.SMWeights[:sum(self.UsedSMNDList)].mul(ReWeighting).split(self.UsedSMNDList)\n",
    "        \n",
    "        self.UsedSMParValList =  [torch.ones(N, dtype=torch.double)*DF.Values for (DF, N) in zip(self.BSMDataFiles, self.UsedSMNDList)]\n",
    "        \n",
    "        self.UsedSMTargetList = [torch.zeros(N, dtype=torch.double) for N in self.UsedSMNDList]\n",
    "\n",
    "####### Join SM with BSM data\n",
    "        self.Data = torch.cat(\n",
    "            [torch.cat([self.UsedSMDataList[i], self.BSMDataList[i]]\n",
    "                                  ) for i in range(len(self.BSMDataList))]\n",
    "            )\n",
    "        self.Weights = torch.cat(\n",
    "            [torch.cat([self.UsedSMWeightsList[i], self.BSMWeightsList[i]]\n",
    "                                  ) for i in range(len(self.BSMWeightsList))]\n",
    "            )\n",
    "        self.Labels = torch.cat(\n",
    "            [torch.cat([self.UsedSMTargetList[i], self.BSMTargetList[i]]\n",
    "                                  ) for i in range(len(self.BSMTargetList))]\n",
    "            )\n",
    "        self.ParVal = torch.cat(\n",
    "            [torch.cat([self.UsedSMParValList[i], self.BSMParValList[i]]\n",
    "                                  ) for i in range(len(self.BSMParValList))]\n",
    "            )\n",
    "        \n",
    "####### Return Tranining Data\n",
    "    def ReturnData(self):\n",
    "        return [self.Data, self.Labels, self.Weights, self.ParVal]\n",
    "            \n",
    "    def Report(self):\n",
    "        #from tabulate import tabulate\n",
    "        print('\\nLoaded SM Files:')\n",
    "        print(tabulate({str(self.Parameters): [ file.Values for file in self.SMDataFiles ], \n",
    "                        \"#Data\":[ file.ND for file in self.SMDataFiles ], \n",
    "                        \"XS[pb](avg.w)\":[ file.XS for file in self.SMDataFiles ]}, headers=\"keys\"))\n",
    "        print('\\nLoaded BSM Files:')\n",
    "        print(tabulate({str(self.Parameters): [ file.Values for file in self.BSMDataFiles ], \n",
    "                        \"#Data\":[ file.ND for file in self.BSMDataFiles ], \n",
    "                        \"XS[pb](avg.w)\":[ file.XS for file in self.BSMDataFiles ]}, headers=\"keys\"))\n",
    "        print('\\nPaired BSM/SM Datasets:\\n')\n",
    "        print(tabulate({str(self.Parameters): [ file.Values for file in self.BSMDataFiles ], \"#Ev.BSM\": self.BSMNDList\n",
    "                        , \"#Ev.SM\": self.UsedSMNDList,\n",
    "                        \"sum.w SM\\/XSSM\": [(self.UsedSMWeightsList[i].sum())/(self.SMWeights.mean()) for i in range(len(self.BSMDataFiles))]\n",
    "                       }, headers=\"keys\"))\n",
    "\n",
    "TD = OurTrainingData(['/data3/Training/TrainingData/GWData/ChPsm.h5', '/data3/MadGraph/testSM1.h5'], \n",
    "                     ['/data3/Training/TrainingData/GWData/ChPgw2.h5', '/data3/Training/TrainingData/GWData/ChPgwm2.h5',\n",
    "                      '/data3/Training/TrainingData/GWData/ChPgw5.h5'],\n",
    "                     ['GW[TeV**-2]'], verbose=False, SMNLimits = [ 10, 5 ], BSMNLimits = [ 2, 3, 4 ] )\n",
    "#print(TD.SMData.size())\n",
    "#print(TD.SMData.dtype)\n",
    "#print([w.size() for w in TD.BSMWeightsList])\n",
    "#print([w.dtype for w in TD.BSMWeightsList])\n",
    "TD.Report()\n",
    "print(TD.Data.size())\n",
    "print(TD.Weights.size())\n",
    "print(TD.Labels.size())\n",
    "print(TD.ParVal.size())\n",
    "\n",
    "print(TD.Labels)\n",
    "\n",
    "print(TD.ParVal)\n",
    "\n",
    "print(TD.Weights)\n",
    "\n",
    "print('%.8f' %TD.Weights[3])\n",
    "\n",
    "print('%.8f' %TD.BSMDataFiles[0].Weights[0])\n",
    "\n",
    "print('%.8f' %TD.Weights[0])\n",
    "\n",
    "print('%.8f' %(TD.SMDataFiles[0].Weights[0]*2./3.))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurTrainingData():\n",
    "    def __init__(self, SMfilepathlist, BSMfilepathlist, parameters, SMNLimits=\"NA\", BSMNLimits=\"NA\", verbose=True): \n",
    "        self.Parameters = parameters\n",
    "        print('Loading Data Files with Parameters: ' + str(parameters) ) \n",
    "        if len(self.Parameters)!= 1: print('Only 1D Implemented in Training !')        \n",
    "####### Load SM data\n",
    "        if type(SMfilepathlist) == list:\n",
    "            if all(isinstance(n, str) for n in SMfilepathlist):\n",
    "                self.SMFilePathList = SMfilepathlist\n",
    "                self.SMNumFiles = len(self.SMFilePathList)\n",
    "                def ReadSMFile(path): \n",
    "                    print('\\nReading SM File ...' + path)\n",
    "                    file = h5py.File(path, 'r')\n",
    "                    if list(file.keys()) == ['Data', 'Info', 'Parameters', 'Values', 'Weights']:\n",
    "                        if file['Parameters'][()] == self.Parameters:\n",
    "                            if file['Values'][()] == [0] * len(file['Values'][()]):\n",
    "                                if verbose: print('##### File Info:\\n' +\n",
    "                                    file['Info'][()][0] + '\\n#####')\n",
    "                                return [file['Info'][()][0], torch.Tensor(file['Values'][()]), torch.Tensor(file['Data'][()]), \n",
    "                                    torch.Tensor(file['Weights'][()])]    \n",
    "                            else:\n",
    "                                print('File: ' + path + ' is of BSM type!')\n",
    "                                return None\n",
    "                        else:\n",
    "                            print('Parameters key is ' + str(file['Parameters'][()]) + '. It should be '+ str(self.Parameters))\n",
    "                            return None\n",
    "                    else:\n",
    "                        print('File format not valid: ' + path)\n",
    "                        return None\n",
    "                ImportedFiles = list(map(ReadSMFile, self.SMFilePathList))\n",
    "                if not None in ImportedFiles:\n",
    "                    self.SMInfoList, self.SMValuesList, self.SMFilesDataList, \\\n",
    "                        self.SMFilesWeightsList = list(map(list, zip(*ImportedFiles)))\n",
    "                    self.SMFilesNDataList =  list(map(len, self.SMFilesDataList))\n",
    "                    self.SMSigmaList = list(map(np.average, self.SMFilesWeightsList))                    \n",
    "            else:\n",
    "                print('SMfilepathlist input should be a list of strings !')\n",
    "        else:\n",
    "            print('SMfilepathlist input should be a list !')\n",
    "\n",
    "####### Join SM data and SM weigths\n",
    "        if type(SMNLimits) == int:\n",
    "            SMNLimits = [SMNLimits for data in self.SMFilesDataList]\n",
    "        elif type(SMNLimits) == list and all(isinstance(n, int) for n in SMNLimits):\n",
    "            if len(SMNLimits) != len(self.SMFilesDataList):\n",
    "                print(\"--> Please input %d integers to chop each SM file.\"%(\n",
    "                    len(self.SMFilesDataList)))\n",
    "            elif sum([self.SMFilesNDataList[i] >= SMNLimits[i] for i in range(len(SMNLimits))]\n",
    "                    ) != len(self.SMFilesNDataList):\n",
    "                print(\"--> Some chop limit larger than available data in the corresponding file.\")\n",
    "                print(\"--> Lengths of the files: \"+str(self.SMFilesNDataList))\n",
    "        else:\n",
    "            SMNLimits = [len(data) for data in self.SMFilesDataList]\n",
    "            \n",
    "        self.SMNDataList = SMNLimits\n",
    "        self.SMDataList = [self.SMFilesDataList[i][:self.SMNDataList[i]] \\\n",
    "                           for i in range(self.SMNumFiles)]\n",
    "        self.SMWeightsList = [self.SMFilesWeightsList[i][:self.SMNDataList[i]] \\\n",
    "                           for i in range(self.SMNumFiles)]\n",
    "####### Join SM Data\n",
    "        self.SMData = torch.cat(self.SMDataList, 0)  \n",
    "        self.SMWeights = torch.cat(self.SMWeightsList, 0)\n",
    "        self.SMNData = sum(self.SMNDataList)\n",
    "####### Load BSM data\n",
    "        if type(BSMfilepathlist) == list:\n",
    "            if all(isinstance(n, str) for n in BSMfilepathlist):\n",
    "                self.BSMFilePathList = BSMfilepathlist\n",
    "                self.BSMNumFiles = len(self.BSMFilePathList)\n",
    "                def ReadBSMFile(path): \n",
    "                    print('\\nReading BSM File ...' + path)\n",
    "                    file = h5py.File(path, 'r')\n",
    "                    if list(file.keys()) == ['Data', 'Info', 'Parameters', 'Values', 'Weights']:\n",
    "                        if file['Parameters'][()] == self.Parameters:\n",
    "                            if file['Values'][()] != [0] * len(file['Values'][()]):\n",
    "                                if verbose: print('##### File Info:\\n' +\n",
    "                                    file['Info'][()][0] + '\\n#####')\n",
    "                                return [file['Info'][()][0], torch.Tensor(file['Values'][()]), torch.Tensor(file['Data'][()]), \n",
    "                                    torch.Tensor(file['Weights'][()])]    \n",
    "                            else:\n",
    "                                print('File: ' + path + ' is of SM type!')\n",
    "                                return None\n",
    "                        else:\n",
    "                            print('Parameters key is ' + str(file['Parameters'][()]) + '. It should be '+ str(self.Parameters))\n",
    "                            return None\n",
    "                    else:\n",
    "                        print('File format not valid: ' + path)\n",
    "                        return None\n",
    "                ImportedFiles = list(map(ReadBSMFile, self.BSMFilePathList))\n",
    "                if not None in ImportedFiles:\n",
    "                    self.BSMInfoList, self.BSMValuesList, self.BSMDataList, \\\n",
    "                        self.BSMWeightsList = list(map(list, zip(*ImportedFiles)))\n",
    "                    self.BSMNDataList = list(map(len, self.BSMDataList))\n",
    "                    self.BSMSigmaList = list(map(np.average, self.BSMWeightsList))\n",
    "            else:\n",
    "                print('BSMfilepathlist input should be a list of strings !')\n",
    "        else:\n",
    "            print('BSMfileathlist input should be a list !')\n",
    "            \n",
    "        if type(BSMNLimits) == int:\n",
    "            BSMNLimits = [BSMNLimits for data in self.BSMDataList]\n",
    "        elif type(BSMNLimits) == list and all(isinstance(n, int) for n in BSMNLimits):\n",
    "            if len(BSMNLimits) != len(self.BSMDataList):\n",
    "                print(\"--> Please input %d integers to chop each SM file.\"%(\n",
    "                    len(self.BSMDataList)))\n",
    "            elif sum([self.BSMNDataList[i] >= BSMNLimits[i] for i in range(len(BSMNLimits))]\n",
    "                    ) != len(self.BSMNDataList):\n",
    "                print(\"--> Some chop limit larger than available data in the corresponding file.\")\n",
    "                print(\"--> Lengths of the files: \"+str(self.BSMNDataList))\n",
    "            else:\n",
    "                self.BSMNDataList = BSMNLimits\n",
    "                self.BSMDataList = [self.BSMDataList[i][:self.BSMNDataList[i]] \\\n",
    "                                   for i in range(len(self.BSMDataList))]\n",
    "####### Break SM data in blocks to be paired with BSM data \n",
    "        BSMNRatioDataList = list(map(lambda n: n/sum(self.BSMNDataList), self.BSMNDataList))\n",
    "        self.SMNSampleList = [int(self.SMNData*BSMNRatioData) for BSMNRatioData in BSMNRatioDataList] \n",
    "        #self.SMValues = torch.cat([torch.ones(self.SMNSampleList[i])*self.BSMValuesList[i]\n",
    "        #                     for i in range(len(BSMNRatioDataList))])\n",
    "        self.SMNSample = sum(self.SMNSampleList)\n",
    "        self.SMSample = self.SMData[: self.SMNSample]\n",
    "        ReWeighting = torch.cat([torch.ones(self.SMNSampleList[i])*self.BSMNDataList[i]/self.SMNSampleList[i] \n",
    "                                 for i in range(len(BSMNRatioDataList))])\n",
    "        #print(ReWeighting)\n",
    "        #print(sum(self.SMWeights))\n",
    "        self.SMWeights = self.SMWeights[:self.SMNSample].mul(ReWeighting)\n",
    "        print((self.SMWeights.sum())/np.average(self.SMWeights))\n",
    "        print((self.SMWeights.sum())/self.SMWeights.mean())\n",
    "        print(len(self.SMWeights))\n",
    "        #print('Number of points in SM samples: '+str(self.SMNSampleList))\n",
    "        \n",
    "        #print('Number of points in BSM samples: '+str(self.BSMNDataList))\n",
    "####### Create training labels\n",
    "        self.SMLabels = torch.zeros(self.SMNSample).split(self.SMNSampleList)\n",
    "        self.BSMLabels = torch.ones(sum(self.BSMNDataList)).split(self.BSMNDataList)\n",
    "####### Create training parameter values\n",
    "        self.ParameterValuesList = [torch.ones(self.SMNSampleList[i] + self.BSMNDataList[i])*\\\n",
    "                                    (self.BSMValuesList[i]) for i in range(len(self.BSMDataList))]\n",
    "####### Join SM with BSM data\n",
    "        self.SMSampleList = torch.split(self.SMSample, self.SMNSampleList, 0)\n",
    "        self.SMWeightsList = torch.split(self.SMWeights, self.SMNSampleList, 0)\n",
    "        self.DataList = [torch.cat([self.SMSampleList[i], self.BSMDataList[i]]\n",
    "                                  ) for i in range(len(self.BSMDataList))]\n",
    "        self.LabelList = [torch.cat([self.SMLabels[i], self.BSMLabels[i]]\n",
    "                                  ) for i in range(len(self.BSMDataList))]\n",
    "        self.WeightsList = [torch.cat([self.SMWeightsList[i], self.BSMWeightsList[i]]\n",
    "                                  ) for i in range(len(self.BSMNDataList))]\n",
    "        self.Data = torch.cat(self.DataList)\n",
    "        self.Labels = torch.cat(self.LabelList)\n",
    "        self.Weights = torch.cat(self.WeightsList)\n",
    "        self.TrainingParameters = torch.cat(self.ParameterValuesList)\n",
    "####### Output Tranining Data\n",
    "    def ReturnData(self):\n",
    "        return [self.Data, self.Labels, self.Weights, self.TrainingParameters]\n",
    "\n",
    "    def Report(self):\n",
    "        #from tabulate import tabulate\n",
    "        print('\\nLoaded Files:\\n')\n",
    "        print(tabulate({str(self.Parameters): self.SMValuesList, \n",
    "                        \"#Events\": self.SMFilesNDataList, \"XS[pb](avg.w)\": self.SMSigmaList}, headers=\"keys\"))\n",
    "        print(' ')\n",
    "        print(tabulate({ str(self.Parameters): self.BSMValuesList, \n",
    "                        \"#Events\": self.BSMNDataList, \"XS[pb](avg.w)\": self.BSMSigmaList}, headers=\"keys\"))\n",
    "        print('\\nPaired BSM/SM Datasets:\\n')\n",
    "        print(tabulate({str(self.Parameters): self.BSMValuesList, \"#Ev.BSM\": self.BSMNDataList\n",
    "                        , \"#Ev.SM\": self.SMNSampleList, \n",
    "                        \"sum.w BSM\\/XSBSM\": [sum(self.BSMWeightsList[i])/self.BSMSigmaList[i] for i in range(len(self.BSMWeightsList))],\n",
    "                        \"sum.w SM\\/XSSM\": [sum(self.SMWeightsList[i])//np.average(self.SMSigmaList) for i in range(len(self.SMWeightsList))]\n",
    "                       }, headers=\"keys\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " t = OurTrainingData(['/data3/Training/TrainingData/GWData/ChMsm.h5'],\\\n",
    "                    ['/data3/Training/TrainingData/GWData/ChMgw10.h5'], ['GW[TeV**-2]'],\n",
    "                    SMNLimits=[19000])\n",
    "t.Report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = OurTrainingData(['/data3/MadGraph/testSM1.h5','/data3/MadGraph/testSM2.h5'],\\\n",
    "                    ['/data3/MadGraph/testBSM1.h5', '/data3/MadGraph/testBSM2.h5'], ['GW'],\n",
    "                   SMNLimits=[5000, 2000], BSMNLimits=[10000, 5000])\n",
    "t.Report()\n",
    "#o = t.ReturnData()\n",
    "#print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.SMDataList[0][:np.infty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.array([1, 2, 3])\n",
    "\n",
    "arr2 = np.array([4, 5, 6])\n",
    "\n",
    "arr = np.concatenate((arr1, arr2))\n",
    "\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 4, 5]\n",
    "b = [0, 1, 3, 5]\n",
    "sum([a[i] > b[i] for i in range(len(a))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurTrainingData():\n",
    "    def __init__(self, filepathlist, parameters = ['GW']): \n",
    "        self.Parameters = parameters\n",
    "        print('Only 1D Implemented in Training !') if len(self.Parameters)!=1 else None\n",
    "        if type(filepathlist) == list:\n",
    "            if all(isinstance(n, str) for n in filepathlist):\n",
    "                self.FilePathList = filepathlist\n",
    "                #print('Reading Files ...')\n",
    "                #print(*self.FilePathList, sep = '\\n')\n",
    "                def ReadFile(path): \n",
    "                    print('Reading File ...' + path)\n",
    "                    file = h5py.File(path, 'r')\n",
    "                    #print(list(file.keys()))\n",
    "                    if list(file.keys()) != ['Data', 'Info', 'Parameters', 'Values', 'Weights']:\n",
    "                        print('File format not valid: ' + path)\n",
    "                        return None\n",
    "                    else:\n",
    "                        return [file['Info'][()][0], np.array(file['Parameters'][()]), np.array(file['Values'][()]), np.array(file['Data'][()]), \n",
    "                                np.array(file['Weights'][()])]\n",
    "                ImportedFiles = list(map(ReadFile, self.FilePathList))\n",
    "                if not None in ImportedFiles:\n",
    "                    self.InfoList, self.ParametersList, self.ValuesList, self.DataList, self.WeigthsList = list(map(list, zip(*ImportedFiles)))\n",
    "                    self.NDataList =  list(map(len, self.DataList))\n",
    "                    def f(x) : \n",
    "                        if x.sum() == 0:\n",
    "                            return 0\n",
    "                        else:\n",
    "                            return 1\n",
    "                    self.TargetList = list(map(f, self.ValuesList))\n",
    "                    if np.all(self.ParametersList !=  self.Parameters): print('Not all files have ' + str(self.Parameters) + 'Parameters !')\n",
    "            else:\n",
    "                print('Input should be a list of strings !')\n",
    "        else:\n",
    "            print('Input should be a list !')\n",
    "    def ReturnData(self):\n",
    "        output = []\n",
    "        for i in range (0, len(self.InfoList)):\n",
    "            print('here' + str(self.TargetList[i]))\n",
    "            targ = np.empty(self.NDataList[i])\n",
    "            targ.fill(self.TargetList[i])\n",
    "            output.extend(targ)\n",
    "        return np.array(output)\n",
    " #       def f(target,):\n",
    "  #          target = x[0]\n",
    "   #         Data = x[1]\n",
    "    #        tl = np.empty(len(Data))\n",
    "     #       tl.fill(target)\n",
    "                \n",
    "    def Report(self):\n",
    "        from tabulate import tabulate\n",
    "        print('Report:')\n",
    "        print(tabulate({\"File\": self.FilePathList, \"Info\": self.InfoList, \"Parameters\": self.ParametersList, \"Values\": self.ValuesList, \n",
    "                        \"Target\": self.TargetList, \"#Events\": self.NDataList}, headers=\"keys\"))\n",
    "        #print(*self.FilePathList, sep = '\\n')\n",
    "        #print(list(map(len, self.DataList)))\n",
    "        #print(list(self.ParametersList))\n",
    "        #print(list(self.ValuesList))       \n",
    "#def Import_pData(datafilepath):\n",
    "#    print(datafilepath)\n",
    "#    datafile = h5py.File(datafilepath, 'r')\n",
    "#    print(list(datafile.keys()))\n",
    "#    if list(datafile.keys()) != ['Data', 'Weights']:\n",
    "#        print('File format not vadid: ' + datafilepath)\n",
    "#        return 1\n",
    "#    else:\n",
    "#        data=datafile['Data']\n",
    "#        weights=datafile['Weights']\n",
    "#        #print(list(weights))\n",
    "#        return np.array([weights,data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0,0.])\n",
    "b = np.array(['f','g'])\n",
    "a.sum()\n",
    "print(a[a.nonzero()])\n",
    "print(b[a.nonzero()])\n",
    "print('here') if True else None\n",
    "a = np.array([[1,2,3],[3,4,5]])\n",
    "len(a)\n",
    "a = []\n",
    "a.append([1])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): return -x, x\n",
    "a, b, = list(map(f,[3,4]))\n",
    "print(a)\n",
    "list(map(list, zip(*[[1,2],[3,4]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor([[1,2,3],[2,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [3, 2]\n",
    "a.sort()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuadraticNet, self).__init__()\n",
    "        self.fclist1  = nn.ModuleList([nn.Linear(n_neurons[i], \n",
    "            n_neurons[i+1]) for i in range(len(n_neurons)-1)])\n",
    "        self.fc1     = nn.Linear(n_neurons[-1], 1)\n",
    "        \n",
    "        self.fclist2  = nn.ModuleList([nn.Linear(n_neurons[i], \n",
    "            n_neurons[i+1]) for i in range(len(n_neurons)-1)])\n",
    "        self.fc2     = nn.Linear(n_neurons[-1], 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, p    = x[:, :input_dim], x[:, input_dim:-1].squeeze() \n",
    "        x1 = x2 = x\n",
    "        \n",
    "        for i, l in enumerate(self.fclist1):\n",
    "            x1 = torch.sigmoid(l(x1))\n",
    "        x1     = self.fc1(x1).squeeze()\n",
    "        \n",
    "        for i, l in enumerate(self.fclist2):\n",
    "            x2 = torch.sigmoid(l(x2))\n",
    "        x2     = self.fc2(x2).squeeze()\n",
    "        \n",
    "        capf   = torch.log(\n",
    "            ((1 + torch.mul(x1, p))**2 + (torch.mul(x2, p))**2))\n",
    "        \n",
    "        return torch.sigmoid(capf).view(-1, 1)\n",
    "    \n",
    "    def get_L1max(self, value_per_unit):\n",
    "        L1_max = []\n",
    "        for m in model_plus.children():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                L1_max.append(m.weight.size(0)*m.weight.size(1) \\\n",
    "                                  * value_per_unit)\n",
    "            else:\n",
    "                for mm in m:\n",
    "                    L1_max.append(mm.weight.size(0)*mm.weight.size(1)\\\n",
    "                                      *value_per_unit)\n",
    "        self.L1max_list = L1_max\n",
    "    \n",
    "    def clip_L1(self):\n",
    "        counter = 0\n",
    "        for m in model_plus.children():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                counter += 1\n",
    "                with torch.no_grad():\n",
    "                    designated_L1 = self.L1max_list[counter-1]\n",
    "                    if designated_L1 == value_per_unit*n_neurons[0]*n_neurons[1]:\n",
    "                        continue                                #skipping first layer\n",
    "                    L1 = m.weight.abs().sum()\n",
    "                    m.weight.masked_scatter_(L1>self.L1max_list[counter-1],\n",
    "                                            m.weight/L1*self.L1max_list[counter-1])\n",
    "            else:\n",
    "                for mm in m:\n",
    "                    counter +=1\n",
    "                    with torch.no_grad():\n",
    "                        designated_L1 = self.L1max_list[counter-1]\n",
    "                        if designated_L1 == value_per_unit*n_neurons[0]*n_neurons[1]:\n",
    "                            continue                            #skipping first layer\n",
    "                        L1 = mm.weight.abs().sum()\n",
    "                        mm.weight.masked_scatter_(L1>designated_L1,\n",
    "                                            mm.weight/L1*designated_L1)\n",
    "    \n",
    "    def calculate_ratio(self, points):\n",
    "        with torch.no_grad():\n",
    "            y = self(points)\n",
    "        return y/(1-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=np.array([[1,2],[4,5]]).transpose()\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=2\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = h5py.File('/data3/MadGraph/test.h5', 'r')\n",
    "list(test3.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = h5py.File('/data3/MadGraph/test.h5', 'r')\n",
    "print(list(test.keys()))\n",
    "data=test['Dataset1']\n",
    "print(list(data))\n",
    "data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### helper functions ####\n",
    "\n",
    "def convert_angles_in_data(data, angle_pos):\n",
    "    nonangle_pos = list(set(range(data.shape[1]))-set(angle_pos))\n",
    "    nonangle_pos.sort()\n",
    "\n",
    "    catdata = torch.cat((data[:, angle_pos].cos_(), \n",
    "                         data[:, angle_pos].sin_(),\n",
    "                         data[:, nonangle_pos]), 1)\n",
    "    \n",
    "    return catdata\n",
    "\n",
    "def append_constant(data, constant):\n",
    "    return torch.cat((data, torch.ones(data.size(0), 1)*float(constant)), 1)\n",
    "\n",
    "def report_ETA(beginning, start, epochs, e, loss):\n",
    "    time_elapsed = time.time() - start\n",
    "    time_left    = str(datetime.timedelta(\n",
    "        seconds=((time.time() - beginning)/(e+1)*(epochs-(e+1)))))\n",
    "    print('Training epoch %s (took %.2f sec, time left %s sec) loss %.8f'%(\n",
    "        e, time_elapsed, time_left, loss))\n",
    "    return time.time()\n",
    "\n",
    "def simpleplot(tsm, tbsm, title, sep, p, deltap):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = plt.subplot()\n",
    "    plt.hist(tsm,  50, alpha=0.5, label='SM')\n",
    "    plt.hist(tbsm, 50, alpha=0.5, label='BSM')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    plt.text(x=0.05, y=0.85, transform=ax.transAxes, \n",
    "         s='sep = %.3f\\np = %.3f +/- %.3f'%(sep, p, deltap), \n",
    "         bbox=dict(facecolor='blue', alpha=0.2))\n",
    "    \n",
    "    plt.savefig(outputfolder + '/' + title+'.pdf')\n",
    "    plt.show()\n",
    "\n",
    "def combine_pm(tsm_plus, tbsm_plus, tsm_minus, tbsm_minus, e):\n",
    "    len_sm    = min(len(tsm_plus), len(tsm_minus))\n",
    "    len_bsm   = min(len(tbsm_plus), len(tbsm_minus))\n",
    "    \n",
    "    tsm       = (tsm_plus[:len_sm] + tsm_minus[:len_sm])\n",
    "    tbsm      = (tbsm_plus[:len_bsm] + tbsm_minus[:len_bsm])\n",
    "    \n",
    "    mu_sm     = tsm.mean().item()\n",
    "    mu_bsm    = tbsm.mean().item()\n",
    "    sigma_sm  = tsm.std().item()\n",
    "    sigma_bsm = tbsm.std().item()\n",
    "    med_sm    = tsm.median().item()\n",
    "    \n",
    "    sep    = (mu_sm - mu_bsm)/sigma_bsm\n",
    "    p      = 1.*len([i for i in tbsm if i > med_sm])/len(tsm)\n",
    "    \n",
    "    delta1 = (p * (1 - p)/min(len_sm, len_bsm))**0.5\n",
    "    delta2 = (sigma_sm/sigma_bsm) * np.exp(-((mu_bsm - mu_sm)**2)/(\n",
    "            2 * sigma_bsm**2))/(2*(n_meas**0.5))\n",
    "    deltap = (delta1**2 + delta2**2)**0.5\n",
    "    \n",
    "    title = '%s, %s%s, combined, %s, N=%d, epochs=%d'%(\n",
    "                     outputheader, str(n_neurons), bsm_op, bsm_test, N, e)\n",
    "\n",
    "    simpleplot(tsm, tbsm, title, sep, p, deltap)\n",
    "    \n",
    "    return (p, deltap)\n",
    "\n",
    "def conclude(title, p_history, outputfolder):\n",
    "    title = 'Test p value history - ' + title\n",
    "    plt.subplots(figsize = (8,4))\n",
    "    plt.xscale('log')\n",
    "    plt.ylim(top = max(map(lambda entry: entry[1], p_history[1:])))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('p value')\n",
    "\n",
    "    x    = list(map(lambda l: l[0], p_history))\n",
    "    y    = list(map(lambda l: l[1], p_history))\n",
    "    yerr = list(map(lambda l: l[2], p_history))\n",
    "    plt.errorbar(x, y, yerr = yerr)\n",
    "    \n",
    "    plt.hlines(y=0.05, xmin=x[0], xmax=x[-1], colors='red')\n",
    "\n",
    "    plt.savefig(outputfolder + title + '.pdf')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    np.savetxt(outputfolder + title + '.csv', p_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_by_constant(data, constant, pos, inplace=True):\n",
    "    updatevalue = torch.mul(data[:, pos], constant).view(-1, 1)\n",
    "    if inplace:\n",
    "        return torch.cat((data[:, :pos], updatevalue, data[:, pos+1:]), 1)\n",
    "    else:\n",
    "        return torch.cat((data[:, :pos], data[:, pos+1:], updatevalue), 1)\n",
    "\n",
    "def take_ratio(data, num_pos, den_pos):\n",
    "    ratio = data[:, num_pos]/data[:, den_pos]\n",
    "    data[:, num_pos] = ratio\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticNet(n_neurons):\n",
    "    def __init__(self):\n",
    "        super(QuadraticNet, self).__init__()\n",
    "        self.fclist1  = nn.ModuleList([nn.Linear(n_neurons[i], \n",
    "            n_neurons[i+1]) for i in range(len(n_neurons)-1)])\n",
    "        self.fc1     = nn.Linear(n_neurons[-1], 1)\n",
    "        \n",
    "        self.fclist2  = nn.ModuleList([nn.Linear(n_neurons[i], \n",
    "            n_neurons[i+1]) for i in range(len(n_neurons)-1)])\n",
    "        self.fc2     = nn.Linear(n_neurons[-1], 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, p    = x[:, :input_dim], x[:, input_dim:-1].squeeze() \n",
    "        x1 = x2 = x\n",
    "        \n",
    "        for i, l in enumerate(self.fclist1):\n",
    "            x1 = torch.sigmoid(l(x1))\n",
    "        x1     = self.fc1(x1).squeeze()\n",
    "        \n",
    "        for i, l in enumerate(self.fclist2):\n",
    "            x2 = torch.sigmoid(l(x2))\n",
    "        x2     = self.fc2(x2).squeeze()\n",
    "        \n",
    "        capf   = torch.log(\n",
    "            ((1 + torch.mul(x1, p))**2 + (torch.mul(x2, p))**2))\n",
    "        \n",
    "        return torch.sigmoid(capf).view(-1, 1)\n",
    "    \n",
    "    def get_L1max(self, value_per_unit):\n",
    "        L1_max = []\n",
    "        for m in model_plus.children():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                L1_max.append(m.weight.size(0)*m.weight.size(1) \\\n",
    "                                  * value_per_unit)\n",
    "            else:\n",
    "                for mm in m:\n",
    "                    L1_max.append(mm.weight.size(0)*mm.weight.size(1)\\\n",
    "                                      *value_per_unit)\n",
    "        self.L1max_list = L1_max\n",
    "    \n",
    "    def clip_L1(self):\n",
    "        counter = 0\n",
    "        for m in model_plus.children():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                counter += 1\n",
    "                with torch.no_grad():\n",
    "                    designated_L1 = self.L1max_list[counter-1]\n",
    "                    if designated_L1 == value_per_unit*n_neurons[0]*n_neurons[1]:\n",
    "                        continue                                #skipping first layer\n",
    "                    L1 = m.weight.abs().sum()\n",
    "                    m.weight.masked_scatter_(L1>self.L1max_list[counter-1],\n",
    "                                            m.weight/L1*self.L1max_list[counter-1])\n",
    "            else:\n",
    "                for mm in m:\n",
    "                    counter +=1\n",
    "                    with torch.no_grad():\n",
    "                        designated_L1 = self.L1max_list[counter-1]\n",
    "                        if designated_L1 == value_per_unit*n_neurons[0]*n_neurons[1]:\n",
    "                            continue                            #skipping first layer\n",
    "                        L1 = mm.weight.abs().sum()\n",
    "                        mm.weight.masked_scatter_(L1>designated_L1,\n",
    "                                            mm.weight/L1*designated_L1)\n",
    "    \n",
    "    def calculate_ratio(self, points):\n",
    "        with torch.no_grad():\n",
    "            y = self(points)\n",
    "        return y/(1-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QuadraticNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticNetReLU(QuadraticNet):\n",
    "    def __init__(self):\n",
    "        super(QuadraticNetReLU, self).__init__()\n",
    "        self.fclist1  = nn.ModuleList([nn.Linear(n_neurons[i], \n",
    "            n_neurons[i+1]) for i in range(len(n_neurons)-1)])\n",
    "        self.fc1     = nn.Linear(n_neurons[-1], 1)\n",
    "        \n",
    "        self.fclist2  = nn.ModuleList([nn.Linear(n_neurons[i], \n",
    "            n_neurons[i+1]) for i in range(len(n_neurons)-1)])\n",
    "        self.fc2     = nn.Linear(n_neurons[-1], 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, p    = x[:, :input_dim], x[:, input_dim:-1].squeeze()\n",
    "        x1 = x2 = x\n",
    "        \n",
    "        for i, l in enumerate(self.fclist1):\n",
    "            x1 = torch.nn.functional.relu(l(x1))\n",
    "        x1     = self.fc1(x1).squeeze()\n",
    "        \n",
    "        for i, l in enumerate(self.fclist2):\n",
    "            x2 = torch.nn.functional.relu(l(x2))\n",
    "        x2     = self.fc2(x2).squeeze()\n",
    "        \n",
    "        capf   = torch.log(\n",
    "            ((1 + torch.mul(x1, p))**2 + (torch.mul(x2, p))**2))\n",
    "        \n",
    "        return torch.sigmoid(capf).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Loss(Module):\n",
    "    def __init__(self, size_average=None, reduce=None, reduction='mean'):\n",
    "        super(_Loss, self).__init__()\n",
    "        if size_average is not None or reduce is not None:\n",
    "            self.reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
    "        else:\n",
    "            self.reduction = reduction\n",
    "\n",
    "class WeightedMSELoss(_Loss):\n",
    "    __constants__ = ['reduction']\n",
    "\n",
    "    def __init__(self, size_average=None, reduce=None, reduction='mean'):\n",
    "        super(WeightedMSELoss, self).__init__(size_average, reduce, reduction)\n",
    "\n",
    "    def forward(self, input, target, weight):\n",
    "        return torch.mean(torch.mul(weight, (input - target)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data - Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train):\n",
    "    model.get_L1max(value_per_unit)\n",
    "    optimiser           = torch.optim.Adam(model.parameters(), lr)\n",
    "    criterion           = WeightedMSELoss()\n",
    "    \n",
    "    if use_gpu:\n",
    "        model                 = model.cuda()\n",
    "        X_train, y_train      = X_train.cuda(), y_train.cuda()\n",
    "\n",
    "    print(\" =================== BEGINNING TRAIN ==================== \")\n",
    "    beginning = start = time.time()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        output          = model(X_train)\n",
    "        loss            = criterion(output, y_train, X_train[:, -1:])\n",
    "\n",
    "        if (e+1) % verbose_prd  == 0:\n",
    "            start       = report_ETA(beginning, start, epochs, e+1, loss)\n",
    "            if save_history:\n",
    "                generictitle = '%s, %s, %s, N=%d, epochs=%d'%(outputheader, pm, str(n_neurons), N, e+1)\n",
    "                torch.save({'state_dict': model.state_dict()}, outputfolder + \n",
    "                                generictitle + '.pth')\n",
    "                modelparams = [w.detach().tolist() for w in model.parameters()]\n",
    "                np.savetxt(outputfolder + generictitle + '.csv', modelparams, '%s')            \n",
    "            \n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        model.clip_L1()\n",
    "        \n",
    "    print(\" ===================   END OF TRAIN   =================== \")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding ratio of PT/pt instead of PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(pm):\n",
    "    sm_file        = sm_filename_fn(pm)    \n",
    "    train_size     = len(bsm_coef)*N\n",
    "    \n",
    "    smdata         = h5py.File(datafolder + sm_filename + '_' + pm + '_nlo.h5', 'r') \n",
    "    nsm            = smdata['Number'][()]\n",
    "    \n",
    "    if isinstance(nsm, np.ndarray):\n",
    "        nsm = nsm[0]\n",
    "    \n",
    "    smdata         = torch.Tensor(smdata['Data'][()])\n",
    "    smdata         = take_ratio(smdata, -2, -3)\n",
    "    \n",
    "    smdata_lst     = [append_constant(smdata[i*N:(i+1)*N, :], bsm_coef[i]\n",
    "                                 ) for i in range(len(bsm_coef))]    # wilson coefficient G for SM\n",
    "    smdata_lst.append(\n",
    "        append_constant(smdata[train_size:, :], bsm_test))           # wilson coefficient G for SM (test)\n",
    "    smdata         = torch.cat(smdata_lst, 0)\n",
    "    \n",
    "    #################################################################################################\n",
    "    #### be careful! as this changes the position of the column of weight and wilson coefficient ####\n",
    "    nlo_w_mean_sm  = smdata[:, -2].mean()\n",
    "    smdata         = multiply_by_constant(smdata, \n",
    "                          1./nlo_w_mean_sm, -2, inplace=False)     # sigma_g/sigma_0 = 1 for SM, NLO\n",
    "    #################################################################################################\n",
    "    \n",
    "    smdata         = append_constant(smdata, 0)                      # training target y = 0 for SM\n",
    "    \n",
    "    nbsm_dic       = {c: (h5py.File(datafolder + bsm_filename_fn(bsm_op, c, pm), \n",
    "                                    'r')['Number'][()]) for c in bsm_coef}\n",
    "        \n",
    "    if isinstance(nbsm_dic[bsm_coef[0]], np.ndarray):\n",
    "        nbsm_dic = {c: torch.as_tensor(nbsm_dic[c][0]) for c in bsm_coef}\n",
    "    else:\n",
    "        nbsm_dic = {c: torch.as_tensor(nbsm_dic[c]) for c in bsm_coef}\n",
    "    \n",
    "    \n",
    "    #nbsm_dic       = {c: (torch.as_tensor(h5py.File(datafolder + bsm_filename(bsm_op, c)\\\n",
    "    #                + '_' + pm + '_nlo.h5', 'r')['Number'][()])) for c in bsm_coef}\n",
    "    \n",
    "    bsmdata_lst    = [torch.Tensor(h5py.File(datafolder + bsm_filename_fn(bsm_op, c, pm),\n",
    "                                             'r')['Data'][()])[:N, :] for c in bsm_coef]\n",
    "    \n",
    "    print('============ nsm: %s ============'%(str(nsm)))\n",
    "    for i in range(len(bsm_coef)):\n",
    "        print('========== nbsm %i: %s =========='%(i, str(nbsm_dic[bsm_coef[i]])))\n",
    "    \n",
    "    bsmdata_lst    = [take_ratio(bsmdata_lst[i], -2, -3) for i in range(len(bsmdata_lst))]\n",
    "\n",
    "    bsmdata_lst    = [append_constant(bsmdata_lst[i], bsm_coef[i]\n",
    "                                 ) for i in range(len(bsmdata_lst))] # wilson coefficient G for BSM\n",
    "    \n",
    "    nlo_w_mean_bsm = [bsmdata_lst[i][:, -2].mean() for i in range(len(bsmdata_lst))]   \n",
    "    \n",
    "    bsmdata_lst    = [multiply_by_constant(bsmdata_lst[i], (nbsm_dic[bsm_coef[i]]/nsm)/nlo_w_mean_bsm[i],\n",
    "                            -2, inplace=False) for i in range(len(bsmdata_lst))] # sigma_g/sigma_0 = nbsm/nsm for BSM, NLO\n",
    "    \n",
    "    bsmdata        = torch.cat(bsmdata_lst, 0)\n",
    "    bsmdata        = append_constant(bsmdata, 1)                     # training target y = 1 for BSM\n",
    "\n",
    "    traindata      = torch.cat((smdata[:train_size, :], bsmdata), 0)\n",
    "    traindata      = traindata[torch.randperm(traindata.size(0))]\n",
    "\n",
    "    if convert_ang:\n",
    "        traindata  = convert_angles_in_data(traindata, angle_pos)\n",
    "\n",
    "    X_train        = traindata[:, :-1] \n",
    "    y_train        = traindata[:, -1].reshape(-1, 1)\n",
    "    \n",
    "    title = 'Scaling (input dim %d, ratio), %s, %s'%(input_dim, pm, bsm_coef)\n",
    "\n",
    "    if os.path.isfile(scalingfolder + title + '.csv'):\n",
    "        print('Reading scaling from: \\n%s...'%(scalingfolder + title + '.csv'))\n",
    "        \n",
    "        scalingPars  = np.loadtxt(open(scalingfolder + title + '.csv', 'r'))\n",
    "        X_train_mean = torch.from_numpy(scalingPars[:input_dim+1]).type(torch.FloatTensor)\n",
    "        X_train_std  = torch.from_numpy(scalingPars[input_dim+1:]).type(torch.FloatTensor)\n",
    "    else:\n",
    "        X_train_mean           = X_train[:,  :input_dim+1].mean(0)\n",
    "        X_train_std            = X_train[:,  :input_dim+1].std(0)\n",
    "    \n",
    "    # normalisation\n",
    "    X_train[:, :input_dim+1] = (X_train[:, :input_dim+1] - X_train_mean)/X_train_std\n",
    "    \n",
    "    #################################################################################################\n",
    "    \n",
    "    bsmdata_test   = h5py.File(datafolder + bsm_filename(bsm_op, bsm_test) +\\\n",
    "                            '_' + pm +'_nlo.h5', 'r')\n",
    "    nbsm_dic[bsm_test] = (bsmdata_test['Number'][()])\n",
    "    \n",
    "    if isinstance(nbsm_dic[bsm_test], np.ndarray):\n",
    "        nbsm_dic[bsm_test] = torch.as_tensor(nbsm_dic[bsm_test][0])\n",
    "    else:\n",
    "        nbsm_dic[bsm_test] = torch.as_tensor(nbsm_dic[bsm_test])\n",
    "    \n",
    "    bsmdata_test   = torch.Tensor(bsmdata_test['Data'][()])\n",
    "    bsmdata_test   = take_ratio(bsmdata_test, -2, -3)\n",
    "    \n",
    "    bsmdata_test   = append_constant(bsmdata_test, bsm_test)         # wilson coefficient G for BSM\n",
    "    \n",
    "    #################################################################################################\n",
    "    #### be careful! as this changes the position of the column of weight and wilson coefficient ####\n",
    "    nlo_w_mean_bsm_test = bsmdata_test[:, -2].mean() \n",
    "    \n",
    "    bsmdata_test   = multiply_by_constant(bsmdata_test, \n",
    "        (nbsm_dic[bsm_test]/nsm)/nlo_w_mean_bsm_test, -2, inplace=False)# sigma_g/sigma_0 = nbsm/nsm for BSM\n",
    "    #################################################################################################\n",
    "    \n",
    "    bsmdata_test   = append_constant(bsmdata_test, 1)                # testing target y = 1 for BSM\n",
    "\n",
    "    if bsm_test in bsm_coef:\n",
    "        bsmdata_test  = bsmdata_test[N:, :]\n",
    "\n",
    "    smdata_test    = smdata[train_size:,  :]\n",
    "    \n",
    "    #testdata       = torch.cat((smdata[train_size:, :], bsmdata_test), 0)\n",
    "    #testdata       = testdata[torch.randperm(testdata.size(0))]\n",
    "\n",
    "    if convert_ang:\n",
    "        smdata_test   = convert_angles_in_data(smdata_test,  angle_pos)\n",
    "        bsmdata_test  = convert_angles_in_data(bsmdata_test, angle_pos)\n",
    "    \n",
    "    X_test_sm      = smdata_test[:,  :-1] \n",
    "    X_test_bsm     = bsmdata_test[:, :-1]\n",
    "    \n",
    "    # normalisation\n",
    "    X_test_sm[:,  :input_dim+1]  = (X_test_sm[:, :input_dim+1] - X_train_mean)/X_train_std\n",
    "    X_test_bsm[:, :input_dim+1] = (X_test_bsm[:, :input_dim+1] - X_train_mean)/X_train_std\n",
    "    \n",
    "    if not os.path.isfile(scalingfolder + title + '.csv'):\n",
    "        print('Saving scaling parameters at: \\n%s...'%(scalingfolder + title + '.csv'))\n",
    "        \n",
    "        np.savetxt(outputfolder + title + '.csv', torch.cat(\n",
    "        [X_train_mean.reshape(-1, 1), X_train_std.reshape(-1, 1)], 0).numpy())\n",
    "        \n",
    "    return X_train, y_train, X_test_sm, X_test_bsm, nsm, nbsm_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafolder    = '/home/chen/Documents/DibosonProcessData_NLO_NewBias/'\n",
    "scalingfolder = '/home/chen/Documents/OutputQuadratic_NLO_NewBias_alt/'\n",
    "outputfolder  = '/home/chen/Documents/OutputQuadratic_NLO_NewBias_alt/'\n",
    "outputheader  = 'Newbias'\n",
    "\n",
    "sm_filename_fn   = lambda pm: 'sm_%s_nlo.h5'%(pm)\n",
    "bsm_filename_fn  = lambda g, c, pm: '%s%s_%s_nlo.h5'%(g, c, pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_pos     = [3, 5]\n",
    "convert_ang   = True\n",
    "\n",
    "use_gpu       = True\n",
    "input_dim     = 8 if not convert_ang else 10\n",
    "n_neurons     = [input_dim, 32, 32, 32]\n",
    "#N             = int(18e4)\n",
    "N             = int(2e5)\n",
    "lr            = 1e-3\n",
    "epochs        = 10000\n",
    "verbose_prd   = 1000\n",
    "n_meas        = 4000\n",
    "value_per_unit= 1.0\n",
    "save_history  = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputheader  = 'NewBias-GW-vanilla'\n",
    "bsm_op        = 'GW'\n",
    "bsm_coef      = ['-1e-7', '-5e-8', '-2e-8', '2e-8', '5e-8', '1e-7']\n",
    "bsm_test      = '2e-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm                  = 'plus'\n",
    "X_train, y_train, X_test_sm, X_test_bsm, nsm, nbsm_dic = read_data(pm)\n",
    "model_plus          = QuadraticNetReLU()\n",
    "model_plus          = train(model_plus, X_train, y_train)\n",
    "\n",
    "pm                    = 'minus'\n",
    "X_train, y_train, X_test_sm, X_test_bsm, nsm, nbsm_dic = read_data(pm)\n",
    "model_minus           = QuadraticNetReLU()\n",
    "model_minus           = train(model_minus, X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
