{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py, torch, time, datetime, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn.modules import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurModel(nn.Module):\n",
    "   \n",
    "    def __init__(self, AR = [1, 3, 3, 1] , AF = 'ReLU' ):               \n",
    "        self.NumberOfEpochs = 100\n",
    "        self.InitialLearningRate = 0.01\n",
    "        self.SaveAfterEpoch = [self.NumberOfEpochs]\n",
    "        self.SaveAfterEpoch = lambda :[self.NumberOfEpochs]  \n",
    "        super(OurModel, self).__init__() \n",
    "        if AF in ['ReLU', 'Sigmoid']:\n",
    "            self.ActivationFunction = AF\n",
    "        else:\n",
    "            print('Valid Activations are: [\\'ReLU\\', \\'Sigmoid\\'].')\n",
    "            self.ActivationFunction = 'ReLU'\n",
    "        if type(AR) == list:\n",
    "            if all(isinstance(n, int) for n in AR):\n",
    "                self.Architecture = AR\n",
    "            else:\n",
    "                print('Architecture should be a list of integers !')\n",
    "                self.Architecture = [1, 3, 3, 1]\n",
    "        else:\n",
    "            print('Architecture should be a list !')\n",
    "            self.Architecture = [1, 3, 3, 1]\n",
    "        self.fclist1  = nn.ModuleList([nn.Linear(self.Architecture[i], \n",
    "        self.Architecture[i+1]) for i in range(len(self.Architecture)-2)])\n",
    "        self.fc1 = nn.Linear(self.Architecture[-2], 1)       \n",
    "        self.fclist2 = nn.ModuleList([nn.Linear(self.Architecture[i], \n",
    "        self.Architecture[i+1]) for i in range(len(self.Architecture)-2)])\n",
    "        self.fc2 = nn.Linear(self.Architecture[-2], 1)\n",
    "        \n",
    "    def SetPreprocess(self, TrainingData, TrainingDataParameters):\n",
    "        self.Scaling = TrainingData.std(0)\n",
    "        self.Shift = TrainingData.mean(0)\n",
    "        self.ParameterScaling = TrainingDataParameters.std(0)\n",
    "\n",
    "    def forward(self, x):       \n",
    "        activation = torch.relu if self.ActivationFunction == 'ReLU' else None\n",
    "        activation = torch.sigmoid if self.ActivationFunction == 'Sigmoid' else activation\n",
    "        input_dim = self.Architecture[0]\n",
    "        x, p    = x[:, :input_dim], x[:, input_dim:-1].squeeze() \n",
    "        \n",
    "        if not hasattr(self, 'Shift'):\n",
    "            print('Please Set Preprocess!')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = (x - self.Shift)/self.Scaling\n",
    "            p = p/self.ParameterScaling\n",
    "        \n",
    "        x1 = x2 = x\n",
    "        \n",
    "        for i, l in enumerate(self.fclist1):\n",
    "            x1 = activation(l(x1))\n",
    "        x1 = self.fc1(x1).squeeze()\n",
    "        \n",
    "        for i, l in enumerate(self.fclist2):\n",
    "            x2 = activation(l(x2))\n",
    "        x2 = self.fc2(x2).squeeze()\n",
    "        \n",
    "        capf   = torch.log(\n",
    "            ((1 + torch.mul(x1, p))**2 + (torch.mul(x2, p))**2))        \n",
    "        return torch.sigmoid(capf).view(-1, 1)\n",
    "    \n",
    "    def get_L1max(self, value_per_unit):\n",
    "        L1_max = []\n",
    "        for m in model_plus.children():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                L1_max.append(m.weight.size(0)*m.weight.size(1) \\\n",
    "                                  * value_per_unit)\n",
    "            else:\n",
    "                for mm in m:\n",
    "                    L1_max.append(mm.weight.size(0)*mm.weight.size(1)\\\n",
    "                                      *value_per_unit)\n",
    "        self.L1max_list = L1_max\n",
    "    \n",
    "    def clip_L1(self):\n",
    "        counter = 0\n",
    "        for m in model_plus.children():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                counter += 1\n",
    "                with torch.no_grad():\n",
    "                    designated_L1 = self.L1max_list[counter-1]\n",
    "                    if designated_L1 == value_per_unit*n_neurons[0]*n_neurons[1]:\n",
    "                        continue                                #skipping first layer\n",
    "                    L1 = m.weight.abs().sum()\n",
    "                    m.weight.masked_scatter_(L1>self.L1max_list[counter-1],\n",
    "                                            m.weight/L1*self.L1max_list[counter-1])\n",
    "            else:\n",
    "                for mm in m:\n",
    "                    counter +=1\n",
    "                    with torch.no_grad():\n",
    "                        designated_L1 = self.L1max_list[counter-1]\n",
    "                        if designated_L1 == value_per_unit*n_neurons[0]*n_neurons[1]:\n",
    "                            continue                            #skipping first layer\n",
    "                        L1 = mm.weight.abs().sum()\n",
    "                        mm.weight.masked_scatter_(L1>designated_L1,\n",
    "                                            mm.weight/L1*designated_L1)\n",
    "    \n",
    "    def calculate_ratio(self, points):\n",
    "        with torch.no_grad():\n",
    "            y = self(points)\n",
    "        return y/(1-y)    \n",
    "    \n",
    "    def SetNumberOfEpochs(self,NE):\n",
    "        self.NumberOfEpochs = NE\n",
    "        #self.SaveAfterEpoch = [self.NumberOfEpochs]\n",
    "    def SetInitialLearningRate(self,ILR):\n",
    "        self.InitialLearningRate = ILR\n",
    "    def SetSaveAfterEpoch(self,SAE):\n",
    "        SAE.sort()\n",
    "        self.SaveAfterEpoch = SAE\n",
    "    def Report(self):\n",
    "        print(\n",
    "        'Architecture = ' + str(self.Architecture) + \n",
    "        '\\nActivation function = ' + self.ActivationFunction +\n",
    "        '\\nInitial learning rate = ' + str(self.InitialLearningRate) +\n",
    "        '\\nNumber of epochs = ' + str(self.NumberOfEpochs) +\n",
    "        '\\nSaving network after epoch(s): ' + str(self.SaveAfterEpoch)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 24, 1]\n",
      "1\n",
      "Sigmoid\n",
      "100\n",
      "100\n",
      "25000\n",
      "[25000]\n",
      "23\n",
      "Architecture = [1, 24, 1]\n",
      "Activation function = Sigmoid\n",
      "Initial learning rate = 23\n",
      "Number of epochs = 25000\n",
      "Saving network after epoch(s): [47, 203]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2780],\n",
       "        [0.6618]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = OurModel([1,24,1] ,'Sigmoid')\n",
    "print(x.Architecture)\n",
    "print(x.Architecture[0])\n",
    "print(x.ActivationFunction)\n",
    "print(x.NumberOfEpochs)\n",
    "print(x.NumberOfEpochs)\n",
    "x.SetNumberOfEpochs(25000)\n",
    "print(x.NumberOfEpochs)\n",
    "print(x.SaveAfterEpoch())\n",
    "x.SetSaveAfterEpoch([203,47])\n",
    "x.SetInitialLearningRate(23)\n",
    "print(x.InitialLearningRate)\n",
    "x.Report()\n",
    "x.SetPreprocess(torch.Tensor([[2],[5]]), torch.Tensor([1, 2]))\n",
    "x.forward(torch.Tensor([[1,2,4],[2,4,5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Loss(Module):\n",
    "    def __init__(self, size_average=None, reduce=None, reduction='mean'):\n",
    "        super(_Loss, self).__init__()\n",
    "        if size_average is not None or reduce is not None:\n",
    "            self.reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
    "        else:\n",
    "            self.reduction = reduction\n",
    "\n",
    "class WeightedMSELoss(_Loss):\n",
    "    __constants__ = ['reduction']\n",
    "\n",
    "    def __init__(self, size_average=None, reduce=None, reduction='mean'):\n",
    "        super(WeightedMSELoss, self).__init__(size_average, reduce, reduction)\n",
    "\n",
    "    def forward(self, input, target, weight):\n",
    "        return torch.mean(torch.mul(weight, (input - target)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurTrainingData():\n",
    "    def __init__(self, filepathlist, ): \n",
    "        if type(filepathlist) == list:\n",
    "            if all(isinstance(n, str) for n in filepathlist):\n",
    "                self.FilePathList = filepathlist\n",
    "                #print('Reading Files ...')\n",
    "                #print(*self.FilePathList, sep = '\\n')\n",
    "                def ReadFile(path): \n",
    "                    print('Reading File ...' + path)\n",
    "                    file = h5py.File(path, 'r')\n",
    "                    #print(list(file.keys()))\n",
    "                    if list(file.keys()) != ['Data', 'Info', 'Parameters', 'Values', 'Weights']:\n",
    "                        print('File format not valid: ' + path)\n",
    "                        return None\n",
    "                    else:\n",
    "                        return [file['Info'][()][0], np.array(file['Parameters'][()]), np.array(file['Values'][()]), file['Data'][()], file['Weights'][()]]\n",
    "                ImportedFiles = list(map(ReadFile,self.FilePathList))\n",
    "                if not None in ImportedFiles:\n",
    "                    self.InfoList, self.ParametersList, self.ValuesList, self.DataList, self.WeigthsList = list(map(list, zip(*ImportedFiles)))\n",
    "                    def f(x) : \n",
    "                        if x.sum() == 0:\n",
    "                            return 0\n",
    "                        else:\n",
    "                            return 1\n",
    "                    self.TargetList = map(f, self.ValuesList)\n",
    "                    if np.all(self.ParametersList == self.ParametersList[0]):\n",
    "                        self.Parameters = self.ParametersList[0]\n",
    "                    else:\n",
    "                        print('Files have different Parameters !')\n",
    "                    def f(x) :\n",
    "                        nonzero = x[0].nonzero()\n",
    "                        return self.Parameters[nonzero]\n",
    "                    print(self.ValuesList[0])\n",
    "            else:\n",
    "                print('Input should be a list of strings !')\n",
    "        else:\n",
    "            print('Input should be a list !')\n",
    "    def Report(self):\n",
    "        from tabulate import tabulate\n",
    "        print('Report:')\n",
    "        print(tabulate({\"File\": self.FilePathList, \"Info\": self.InfoList, \"Parameters\": self.ParametersList, \"Values\": self.ValuesList, \n",
    "                        \"Target\": self.TargetList, \"#Events\": list(map(len, self.DataList))}, headers=\"keys\"))\n",
    "        #print(*self.FilePathList, sep = '\\n')\n",
    "        #print(list(map(len, self.DataList)))\n",
    "        #print(list(self.ParametersList))\n",
    "        #print(list(self.ValuesList))       \n",
    "#def Import_pData(datafilepath):\n",
    "#    print(datafilepath)\n",
    "#    datafile = h5py.File(datafilepath, 'r')\n",
    "#    print(list(datafile.keys()))\n",
    "#    if list(datafile.keys()) != ['Data', 'Weights']:\n",
    "#        print('File format not vadid: ' + datafilepath)\n",
    "#        return 1\n",
    "#    else:\n",
    "#        data=datafile['Data']\n",
    "#        weights=datafile['Weights']\n",
    "#        #print(list(weights))\n",
    "#        return np.array([weights,data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurTrainingData():\n",
    "    def __init__(self, SMfilepathlist, BSMfilepathlist, parameters ): \n",
    "        self.Parameters = parameters\n",
    "        print('Only 1D Implemented in Training !') if len(self.Parameters)!= 1 else print('Loading Data with Parameters: ' + str(parameters) )       \n",
    "####### Load SM data\n",
    "        if type(SMfilepathlist) == list:\n",
    "            if all(isinstance(n, str) for n in SMfilepathlist):\n",
    "                self.SMFilePathList = SMfilepathlist\n",
    "                self.SMNumFiles = len(self.SMFilePathList)\n",
    "                #print('Reading Files ...')\n",
    "                #print(*self.FilePathList, sep = '\\n')\n",
    "                def ReadSMFile(path): \n",
    "                    print('Reading SM File ...' + path)\n",
    "                    file = h5py.File(path, 'r')\n",
    "                    #print(list(file.keys()))\n",
    "                    #print(file['Values'][()])\n",
    "                    #print([0] * len(file['Values'][()]))\n",
    "                    if list(file.keys()) == ['Data', 'Info', 'Parameters', 'Values', 'Weights']:\n",
    "                        if file['Values'][()] == [0] * len(file['Values'][()]):\n",
    "                            return [file['Info'][()][0], np.array(file['Parameters'][()]), torch.Tensor(file['Values'][()]), torch.Tensor(file['Data'][()]), \n",
    "                                torch.Tensor(file['Weights'][()])]    \n",
    "                        else:\n",
    "                            print('File: ' + path + ' is of BSM type!')\n",
    "                            return None\n",
    "                    else:\n",
    "                        print('File format not valid: ' + path)\n",
    "                        return None\n",
    "                ImportedFiles = list(map(ReadSMFile, self.SMFilePathList))\n",
    "                if not None in ImportedFiles:\n",
    "                    self.SMInfoList, self.SMParametersList, self.SMValuesList, self.SMDataList, self.SMWeightsList = list(map(list, zip(*ImportedFiles)))\n",
    "                    self.SMNDataList =  list(map(len, self.SMDataList))\n",
    "                    #def f(x) : \n",
    "                    #    if x.sum() == 0:\n",
    "                    #        return 0\n",
    "                    #    else:\n",
    "                    #        return 1\n",
    "                    #self.TargetList = list(map(f, self.ValuesList))\n",
    "                    #if np.all(self.ParametersList !=  self.Parameters): print('Not all files have ' + str(self.Parameters) + 'Parameters !')\n",
    "            else:\n",
    "                print('SMfilefathlist input should be a list of strings !')\n",
    "        else:\n",
    "            print('SMfilefathlist input should be a list !')\n",
    "\n",
    "####### Join SM data and SM weigths\n",
    "        self.SMData = torch.cat(self.SMDataList, 0)  \n",
    "        self.SMWeights = torch.cat(self.SMWeightsList, 0)\n",
    "        self.SMNData = sum(self.SMNDataList)\n",
    "####### Load BSM data\n",
    "        if type(BSMfilepathlist) == list:\n",
    "            if all(isinstance(n, str) for n in BSMfilepathlist):\n",
    "                self.BSMFilePathList = BSMfilepathlist\n",
    "                self.BSMNumFiles = len(self.BSMFilePathList)\n",
    "                #print('Reading Files ...')\n",
    "                #print(*self.FilePathList, sep = '\\n')\n",
    "                def ReadBSMFile(path): \n",
    "                    print('Reading BSM File ...' + path)\n",
    "                    file = h5py.File(path, 'r')\n",
    "                    if list(file.keys()) == ['Data', 'Info', 'Parameters', 'Values', 'Weights']:\n",
    "                        if file['Values'][()] != [0] * len(file['Values'][()]):\n",
    "                            return [file['Info'][()][0], np.array(file['Parameters'][()]), torch.Tensor(file['Values'][()]), torch.Tensor(file['Data'][()]), \n",
    "                                torch.Tensor(file['Weights'][()])]    \n",
    "                        else:\n",
    "                            print('File: ' + path + ' is of SM type!')\n",
    "                            return None\n",
    "                    else:\n",
    "                        print('File format not valid: ' + path)\n",
    "                        return None\n",
    "                ImportedFiles = list(map(ReadBSMFile, self.BSMFilePathList))\n",
    "                if not None in ImportedFiles:\n",
    "                    self.BSMInfoList, self.BSMParametersList, self.BSMValuesList, self.BSMDataList, self.BSMWeightsList = list(map(list, zip(*ImportedFiles)))\n",
    "                    self.BSMNDataList =  list(map(len, self.BSMDataList))\n",
    "                    #def f(x) : \n",
    "                    #    if x.sum() == 0:\n",
    "                    #        return 0\n",
    "                    #    else:\n",
    "                    #        return 1\n",
    "                    #self.TargetList = list(map(f, self.ValuesList))\n",
    "                    #if np.all(self.ParametersList !=  self.Parameters): print('Not all files have ' + str(self.Parameters) + 'Parameters !')\n",
    "            else:\n",
    "                print('BSMfilefathlist input should be a list of strings !')\n",
    "        else:\n",
    "            print('BSMfilefathlist input should be a list !')\n",
    "####### Prepare SM data\n",
    "        BSMNRatioDataList = list(map(lambda n: n/sum(self.BSMNDataList), self.BSMNDataList))\n",
    "        self.SMSampleSizeList = [int(self.SMNData*BSMNRatioData) for BSMNRatioData in BSMNRatioDataList] \n",
    "        self.SMValues = torch.cat([torch.ones(self.SMSampleSizeList[i])*self.BSMValuesList[i]\n",
    "                             for i in range(len(BSMNRatioDataList))])\n",
    "        self.SMNData = sum(self.SMSampleSizeList)\n",
    "        self.SMData = self.SMData[: self.SMNData]\n",
    "        ReWeighting = torch.cat([torch.ones(self.SMSampleSizeList[i])*self.BSMNDataList[i]/self.SMSampleSizeList[i] \n",
    "                                 for i in range(len(BSMNRatioDataList))])\n",
    "        self.SMWeights = self.SMWeights[:self.SMNData].mul(ReWeighting)\n",
    "####### Labels, join BSM and SM\n",
    "####### Allow taking less data\n",
    "    def ReturnData(self):\n",
    "        output = []\n",
    "        for i in range (0, len(self.InfoList)):\n",
    "            print('here' + str(self.TargetList[i]))\n",
    "            targ = np.empty(self.NDataList[i])\n",
    "            targ.fill(self.TargetList[i])\n",
    "            output.extend(targ)\n",
    "        return np.array(output)\n",
    " #       def f(target,):\n",
    "  #          target = x[0]\n",
    "   #         Data = x[1]\n",
    "    #        tl = np.empty(len(Data))\n",
    "     #       tl.fill(target)\n",
    "                \n",
    "    def Report(self):\n",
    "        from tabulate import tabulate\n",
    "        print('Report:')\n",
    "        print(tabulate({\"SM File\": self.SMFilePathList, \"Info\": self.SMInfoList, \"Parameters\": self.SMParametersList, \"Values\": self.SMValuesList, \n",
    "                        #\"Target\": self.TargetList,\n",
    "                        \"#Events\": self.SMNDataList}, headers=\"keys\"))\n",
    "        print(tabulate({\"BSM File\": self.BSMFilePathList, \"Info\": self.BSMInfoList, \"Parameters\": self.BSMParametersList, \"Values\": self.BSMValuesList, \n",
    "                        #\"Target\": self.TargetList,\n",
    "                        \"#Events\": self.BSMNDataList}, headers=\"keys\"))\n",
    "        #print(*self.FilePathList, sep = '\\n')\n",
    "        #print(list(map(len, self.DataList)))\n",
    "        #print(list(self.ParametersList))\n",
    "        #print(list(self.ValuesList))       \n",
    "#def Import_pData(datafilepath):\n",
    "#    print(datafilepath)\n",
    "#    datafile = h5py.File(datafilepath, 'r')\n",
    "#    print(list(datafile.keys()))\n",
    "#    if list(datafile.keys()) != ['Data', 'Weights']:\n",
    "#        print('File format not vadid: ' + datafilepath)\n",
    "#        return 1\n",
    "#    else:\n",
    "#        data=datafile['Data']\n",
    "#        weights=datafile['Weights']\n",
    "#        #print(list(weights))\n",
    "#        return np.array([weights,data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data with Parameters: ['GW']\n",
      "Reading SM File .../data3/MadGraph/testSM1.h5\n",
      "Reading SM File .../data3/MadGraph/testSM2.h5\n",
      "Reading BSM File .../data3/MadGraph/testBSM1.h5\n",
      "Reading BSM File .../data3/MadGraph/testBSM2.h5\n",
      "[0.9052491946181542, 0.09475080538184574]\n",
      "tensor([1.8210e-07, 1.8210e-07, 1.8210e-07,  ..., 1.8210e-07, 1.8210e-07,\n",
      "        1.8210e-07])\n",
      "tensor([1.0498, 1.0498, 1.0498,  ..., 1.0499, 1.0499, 1.0499])\n",
      "tensor([1.9116e-07, 1.9116e-07, 1.9116e-07,  ..., 1.9118e-07, 1.9118e-07,\n",
      "        1.9118e-07])\n",
      "tensor([ 0.1000,  0.1000,  0.1000,  ..., -0.3000, -0.3000, -0.3000])\n",
      "torch.Size([20107])\n",
      "Report:\n",
      "SM File                     Info    Parameters      Values    #Events\n",
      "--------------------------  ------  ------------  --------  ---------\n",
      "/data3/MadGraph/testSM1.h5  blabla  ['GW']               0      19108\n",
      "/data3/MadGraph/testSM2.h5  blabla  ['GW']               0       1000\n",
      "BSM File                     Info    Parameters      Values    #Events\n",
      "---------------------------  ------  ------------  --------  ---------\n",
      "/data3/MadGraph/testBSM1.h5  blabla  ['GW']             0.1      19108\n",
      "/data3/MadGraph/testBSM2.h5  blabla  ['GW']            -0.3       2000\n"
     ]
    }
   ],
   "source": [
    "t = OurTrainingData(['/data3/MadGraph/testSM1.h5','/data3/MadGraph/testSM2.h5'],['/data3/MadGraph/testBSM1.h5', '/data3/MadGraph/testBSM2.h5'], ['GW'])\n",
    "t.Report()\n",
    "#o = t.ReturnData()\n",
    "#print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "arr1 = np.array([1, 2, 3])\n",
    "\n",
    "arr2 = np.array([4, 5, 6])\n",
    "\n",
    "arr = np.concatenate((arr1, arr2))\n",
    "\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurTrainingData():\n",
    "    def __init__(self, filepathlist, parameters = ['GW']): \n",
    "        self.Parameters = parameters\n",
    "        print('Only 1D Implemented in Training !') if len(self.Parameters)!=1 else None\n",
    "        if type(filepathlist) == list:\n",
    "            if all(isinstance(n, str) for n in filepathlist):\n",
    "                self.FilePathList = filepathlist\n",
    "                #print('Reading Files ...')\n",
    "                #print(*self.FilePathList, sep = '\\n')\n",
    "                def ReadFile(path): \n",
    "                    print('Reading File ...' + path)\n",
    "                    file = h5py.File(path, 'r')\n",
    "                    #print(list(file.keys()))\n",
    "                    if list(file.keys()) != ['Data', 'Info', 'Parameters', 'Values', 'Weights']:\n",
    "                        print('File format not valid: ' + path)\n",
    "                        return None\n",
    "                    else:\n",
    "                        return [file['Info'][()][0], np.array(file['Parameters'][()]), np.array(file['Values'][()]), np.array(file['Data'][()]), \n",
    "                                np.array(file['Weights'][()])]\n",
    "                ImportedFiles = list(map(ReadFile, self.FilePathList))\n",
    "                if not None in ImportedFiles:\n",
    "                    self.InfoList, self.ParametersList, self.ValuesList, self.DataList, self.WeigthsList = list(map(list, zip(*ImportedFiles)))\n",
    "                    self.NDataList =  list(map(len, self.DataList))\n",
    "                    def f(x) : \n",
    "                        if x.sum() == 0:\n",
    "                            return 0\n",
    "                        else:\n",
    "                            return 1\n",
    "                    self.TargetList = list(map(f, self.ValuesList))\n",
    "                    if np.all(self.ParametersList !=  self.Parameters): print('Not all files have ' + str(self.Parameters) + 'Parameters !')\n",
    "            else:\n",
    "                print('Input should be a list of strings !')\n",
    "        else:\n",
    "            print('Input should be a list !')\n",
    "    def ReturnData(self):\n",
    "        output = []\n",
    "        for i in range (0, len(self.InfoList)):\n",
    "            print('here' + str(self.TargetList[i]))\n",
    "            targ = np.empty(self.NDataList[i])\n",
    "            targ.fill(self.TargetList[i])\n",
    "            output.extend(targ)\n",
    "        return np.array(output)\n",
    " #       def f(target,):\n",
    "  #          target = x[0]\n",
    "   #         Data = x[1]\n",
    "    #        tl = np.empty(len(Data))\n",
    "     #       tl.fill(target)\n",
    "                \n",
    "    def Report(self):\n",
    "        from tabulate import tabulate\n",
    "        print('Report:')\n",
    "        print(tabulate({\"File\": self.FilePathList, \"Info\": self.InfoList, \"Parameters\": self.ParametersList, \"Values\": self.ValuesList, \n",
    "                        \"Target\": self.TargetList, \"#Events\": self.NDataList}, headers=\"keys\"))\n",
    "        #print(*self.FilePathList, sep = '\\n')\n",
    "        #print(list(map(len, self.DataList)))\n",
    "        #print(list(self.ParametersList))\n",
    "        #print(list(self.ValuesList))       \n",
    "#def Import_pData(datafilepath):\n",
    "#    print(datafilepath)\n",
    "#    datafile = h5py.File(datafilepath, 'r')\n",
    "#    print(list(datafile.keys()))\n",
    "#    if list(datafile.keys()) != ['Data', 'Weights']:\n",
    "#        print('File format not vadid: ' + datafilepath)\n",
    "#        return 1\n",
    "#    else:\n",
    "#        data=datafile['Data']\n",
    "#        weights=datafile['Weights']\n",
    "#        #print(list(weights))\n",
    "#        return np.array([weights,data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "here\n",
      "[[1]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0,0.])\n",
    "b = np.array(['f','g'])\n",
    "a.sum()\n",
    "print(a[a.nonzero()])\n",
    "print(b[a.nonzero()])\n",
    "print('here') if True else None\n",
    "a = np.array([[1,2,3],[3,4,5]])\n",
    "len(a)\n",
    "a = []\n",
    "a.append([1])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-3, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 3], [2, 4]]"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x): return -x, x\n",
    "a, b, = list(map(f,[3,4]))\n",
    "print(a)\n",
    "list(map(list, zip(*[[1,2],[3,4]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name      Age\n",
      "------  -----\n",
      "Alice      24\n",
      "Bob        19\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [2., 4., 5.]])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([[1,2,3],[2,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3]\n"
     ]
    }
   ],
   "source": [
    "a = [3, 2]\n",
    "a.sort()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([1.59062308e+06, 7.38849819e-01, 1.11762201e+00, 3.08047003e+00,\n",
       "       1.31599059e+00, 3.50028196e+00, 4.13548874e+02, 2.37012655e+01,\n",
       "       4.18555177e+02, 8.01572954e+03, 6.37188323e+03]),\n",
       "       array([3.68781055e+06, 3.83362445e-01, 1.53083097e+00, 1.67333005e+00,\n",
       "       4.64560989e-01, 2.70386282e-01, 3.15489925e+02, 5.96262828e+01,\n",
       "       3.46105482e+02, 7.86621494e+03, 6.37189199e+03])], dtype=object)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.821e-07, 1.821e-07, 1.821e-07], dtype=object)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuadraticNet, self).__init__()\n",
    "        self.fclist1  = nn.ModuleList([nn.Linear(n_neurons[i], \n",
    "            n_neurons[i+1]) for i in range(len(n_neurons)-1)])\n",
    "        self.fc1     = nn.Linear(n_neurons[-1], 1)\n",
    "        \n",
    "        self.fclist2  = nn.ModuleList([nn.Linear(n_neurons[i], \n",
    "            n_neurons[i+1]) for i in range(len(n_neurons)-1)])\n",
    "        self.fc2     = nn.Linear(n_neurons[-1], 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, p    = x[:, :input_dim], x[:, input_dim:-1].squeeze() \n",
    "        x1 = x2 = x\n",
    "        \n",
    "        for i, l in enumerate(self.fclist1):\n",
    "            x1 = torch.sigmoid(l(x1))\n",
    "        x1     = self.fc1(x1).squeeze()\n",
    "        \n",
    "        for i, l in enumerate(self.fclist2):\n",
    "            x2 = torch.sigmoid(l(x2))\n",
    "        x2     = self.fc2(x2).squeeze()\n",
    "        \n",
    "        capf   = torch.log(\n",
    "            ((1 + torch.mul(x1, p))**2 + (torch.mul(x2, p))**2))\n",
    "        \n",
    "        return torch.sigmoid(capf).view(-1, 1)\n",
    "    \n",
    "    def get_L1max(self, value_per_unit):\n",
    "        L1_max = []\n",
    "        for m in model_plus.children():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                L1_max.append(m.weight.size(0)*m.weight.size(1) \\\n",
    "                                  * value_per_unit)\n",
    "            else:\n",
    "                for mm in m:\n",
    "                    L1_max.append(mm.weight.size(0)*mm.weight.size(1)\\\n",
    "                                      *value_per_unit)\n",
    "        self.L1max_list = L1_max\n",
    "    \n",
    "    def clip_L1(self):\n",
    "        counter = 0\n",
    "        for m in model_plus.children():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                counter += 1\n",
    "                with torch.no_grad():\n",
    "                    designated_L1 = self.L1max_list[counter-1]\n",
    "                    if designated_L1 == value_per_unit*n_neurons[0]*n_neurons[1]:\n",
    "                        continue                                #skipping first layer\n",
    "                    L1 = m.weight.abs().sum()\n",
    "                    m.weight.masked_scatter_(L1>self.L1max_list[counter-1],\n",
    "                                            m.weight/L1*self.L1max_list[counter-1])\n",
    "            else:\n",
    "                for mm in m:\n",
    "                    counter +=1\n",
    "                    with torch.no_grad():\n",
    "                        designated_L1 = self.L1max_list[counter-1]\n",
    "                        if designated_L1 == value_per_unit*n_neurons[0]*n_neurons[1]:\n",
    "                            continue                            #skipping first layer\n",
    "                        L1 = mm.weight.abs().sum()\n",
    "                        mm.weight.masked_scatter_(L1>designated_L1,\n",
    "                                            mm.weight/L1*designated_L1)\n",
    "    \n",
    "    def calculate_ratio(self, points):\n",
    "        with torch.no_grad():\n",
    "            y = self(points)\n",
    "        return y/(1-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 4]\n",
      " [2 5]]\n"
     ]
    }
   ],
   "source": [
    "l=np.array([[1,2],[4,5]]).transpose()\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "f=2\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data', 'Nev13']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test3 = h5py.File('/data3/MadGraph/test.h5', 'r')\n",
    "list(test3.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dataset1']\n",
      "[1, 2, 3, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = h5py.File('/data3/MadGraph/test.h5', 'r')\n",
    "print(list(test.keys()))\n",
    "data=test['Dataset1']\n",
    "print(list(data))\n",
    "data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### helper functions ####\n",
    "\n",
    "def convert_angles_in_data(data, angle_pos):\n",
    "    nonangle_pos = list(set(range(data.shape[1]))-set(angle_pos))\n",
    "    nonangle_pos.sort()\n",
    "\n",
    "    catdata = torch.cat((data[:, angle_pos].cos_(), \n",
    "                         data[:, angle_pos].sin_(),\n",
    "                         data[:, nonangle_pos]), 1)\n",
    "    \n",
    "    return catdata\n",
    "\n",
    "def append_constant(data, constant):\n",
    "    return torch.cat((data, torch.ones(data.size(0), 1)*float(constant)), 1)\n",
    "\n",
    "def report_ETA(beginning, start, epochs, e, loss):\n",
    "    time_elapsed = time.time() - start\n",
    "    time_left    = str(datetime.timedelta(\n",
    "        seconds=((time.time() - beginning)/(e+1)*(epochs-(e+1)))))\n",
    "    print('Training epoch %s (took %.2f sec, time left %s sec) loss %.8f'%(\n",
    "        e, time_elapsed, time_left, loss))\n",
    "    return time.time()\n",
    "\n",
    "def simpleplot(tsm, tbsm, title, sep, p, deltap):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = plt.subplot()\n",
    "    plt.hist(tsm,  50, alpha=0.5, label='SM')\n",
    "    plt.hist(tbsm, 50, alpha=0.5, label='BSM')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    plt.text(x=0.05, y=0.85, transform=ax.transAxes, \n",
    "         s='sep = %.3f\\np = %.3f +/- %.3f'%(sep, p, deltap), \n",
    "         bbox=dict(facecolor='blue', alpha=0.2))\n",
    "    \n",
    "    plt.savefig(outputfolder + '/' + title+'.pdf')\n",
    "    plt.show()\n",
    "\n",
    "def combine_pm(tsm_plus, tbsm_plus, tsm_minus, tbsm_minus, e):\n",
    "    len_sm    = min(len(tsm_plus), len(tsm_minus))\n",
    "    len_bsm   = min(len(tbsm_plus), len(tbsm_minus))\n",
    "    \n",
    "    tsm       = (tsm_plus[:len_sm] + tsm_minus[:len_sm])\n",
    "    tbsm      = (tbsm_plus[:len_bsm] + tbsm_minus[:len_bsm])\n",
    "    \n",
    "    mu_sm     = tsm.mean().item()\n",
    "    mu_bsm    = tbsm.mean().item()\n",
    "    sigma_sm  = tsm.std().item()\n",
    "    sigma_bsm = tbsm.std().item()\n",
    "    med_sm    = tsm.median().item()\n",
    "    \n",
    "    sep    = (mu_sm - mu_bsm)/sigma_bsm\n",
    "    p      = 1.*len([i for i in tbsm if i > med_sm])/len(tsm)\n",
    "    \n",
    "    delta1 = (p * (1 - p)/min(len_sm, len_bsm))**0.5\n",
    "    delta2 = (sigma_sm/sigma_bsm) * np.exp(-((mu_bsm - mu_sm)**2)/(\n",
    "            2 * sigma_bsm**2))/(2*(n_meas**0.5))\n",
    "    deltap = (delta1**2 + delta2**2)**0.5\n",
    "    \n",
    "    title = '%s, %s%s, combined, %s, N=%d, epochs=%d'%(\n",
    "                     outputheader, str(n_neurons), bsm_op, bsm_test, N, e)\n",
    "\n",
    "    simpleplot(tsm, tbsm, title, sep, p, deltap)\n",
    "    \n",
    "    return (p, deltap)\n",
    "\n",
    "def conclude(title, p_history, outputfolder):\n",
    "    title = 'Test p value history - ' + title\n",
    "    plt.subplots(figsize = (8,4))\n",
    "    plt.xscale('log')\n",
    "    plt.ylim(top = max(map(lambda entry: entry[1], p_history[1:])))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('p value')\n",
    "\n",
    "    x    = list(map(lambda l: l[0], p_history))\n",
    "    y    = list(map(lambda l: l[1], p_history))\n",
    "    yerr = list(map(lambda l: l[2], p_history))\n",
    "    plt.errorbar(x, y, yerr = yerr)\n",
    "    \n",
    "    plt.hlines(y=0.05, xmin=x[0], xmax=x[-1], colors='red')\n",
    "\n",
    "    plt.savefig(outputfolder + title + '.pdf')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    np.savetxt(outputfolder + title + '.csv', p_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_by_constant(data, constant, pos, inplace=True):\n",
    "    updatevalue = torch.mul(data[:, pos], constant).view(-1, 1)\n",
    "    if inplace:\n",
    "        return torch.cat((data[:, :pos], updatevalue, data[:, pos+1:]), 1)\n",
    "    else:\n",
    "        return torch.cat((data[:, :pos], data[:, pos+1:], updatevalue), 1)\n",
    "\n",
    "def take_ratio(data, num_pos, den_pos):\n",
    "    ratio = data[:, num_pos]/data[:, den_pos]\n",
    "    data[:, num_pos] = ratio\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_neurons' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-d5b5068cb44a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mQuadraticNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neurons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQuadraticNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         self.fclist1  = nn.ModuleList([nn.Linear(n_neurons[i], \n\u001b[1;32m      5\u001b[0m             n_neurons[i+1]) for i in range(len(n_neurons)-1)])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_neurons' is not defined"
     ]
    }
   ],
   "source": [
    "class QuadraticNet(n_neurons):\n",
    "    def __init__(self):\n",
    "        super(QuadraticNet, self).__init__()\n",
    "        self.fclist1  = nn.ModuleList([nn.Linear(n_neurons[i], \n",
    "            n_neurons[i+1]) for i in range(len(n_neurons)-1)])\n",
    "        self.fc1     = nn.Linear(n_neurons[-1], 1)\n",
    "        \n",
    "        self.fclist2  = nn.ModuleList([nn.Linear(n_neurons[i], \n",
    "            n_neurons[i+1]) for i in range(len(n_neurons)-1)])\n",
    "        self.fc2     = nn.Linear(n_neurons[-1], 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, p    = x[:, :input_dim], x[:, input_dim:-1].squeeze() \n",
    "        x1 = x2 = x\n",
    "        \n",
    "        for i, l in enumerate(self.fclist1):\n",
    "            x1 = torch.sigmoid(l(x1))\n",
    "        x1     = self.fc1(x1).squeeze()\n",
    "        \n",
    "        for i, l in enumerate(self.fclist2):\n",
    "            x2 = torch.sigmoid(l(x2))\n",
    "        x2     = self.fc2(x2).squeeze()\n",
    "        \n",
    "        capf   = torch.log(\n",
    "            ((1 + torch.mul(x1, p))**2 + (torch.mul(x2, p))**2))\n",
    "        \n",
    "        return torch.sigmoid(capf).view(-1, 1)\n",
    "    \n",
    "    def get_L1max(self, value_per_unit):\n",
    "        L1_max = []\n",
    "        for m in model_plus.children():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                L1_max.append(m.weight.size(0)*m.weight.size(1) \\\n",
    "                                  * value_per_unit)\n",
    "            else:\n",
    "                for mm in m:\n",
    "                    L1_max.append(mm.weight.size(0)*mm.weight.size(1)\\\n",
    "                                      *value_per_unit)\n",
    "        self.L1max_list = L1_max\n",
    "    \n",
    "    def clip_L1(self):\n",
    "        counter = 0\n",
    "        for m in model_plus.children():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                counter += 1\n",
    "                with torch.no_grad():\n",
    "                    designated_L1 = self.L1max_list[counter-1]\n",
    "                    if designated_L1 == value_per_unit*n_neurons[0]*n_neurons[1]:\n",
    "                        continue                                #skipping first layer\n",
    "                    L1 = m.weight.abs().sum()\n",
    "                    m.weight.masked_scatter_(L1>self.L1max_list[counter-1],\n",
    "                                            m.weight/L1*self.L1max_list[counter-1])\n",
    "            else:\n",
    "                for mm in m:\n",
    "                    counter +=1\n",
    "                    with torch.no_grad():\n",
    "                        designated_L1 = self.L1max_list[counter-1]\n",
    "                        if designated_L1 == value_per_unit*n_neurons[0]*n_neurons[1]:\n",
    "                            continue                            #skipping first layer\n",
    "                        L1 = mm.weight.abs().sum()\n",
    "                        mm.weight.masked_scatter_(L1>designated_L1,\n",
    "                                            mm.weight/L1*designated_L1)\n",
    "    \n",
    "    def calculate_ratio(self, points):\n",
    "        with torch.no_grad():\n",
    "            y = self(points)\n",
    "        return y/(1-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_neurons' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-4ce20b6777bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mQuadraticNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-162-fd129a27c35a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQuadraticNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         self.fclist1  = nn.ModuleList([nn.Linear(n_neurons[i], \n\u001b[0;32m----> 5\u001b[0;31m             n_neurons[i+1]) for i in range(len(n_neurons)-1)])\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neurons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_neurons' is not defined"
     ]
    }
   ],
   "source": [
    "QuadraticNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticNetReLU(QuadraticNet):\n",
    "    def __init__(self):\n",
    "        super(QuadraticNetReLU, self).__init__()\n",
    "        self.fclist1  = nn.ModuleList([nn.Linear(n_neurons[i], \n",
    "            n_neurons[i+1]) for i in range(len(n_neurons)-1)])\n",
    "        self.fc1     = nn.Linear(n_neurons[-1], 1)\n",
    "        \n",
    "        self.fclist2  = nn.ModuleList([nn.Linear(n_neurons[i], \n",
    "            n_neurons[i+1]) for i in range(len(n_neurons)-1)])\n",
    "        self.fc2     = nn.Linear(n_neurons[-1], 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, p    = x[:, :input_dim], x[:, input_dim:-1].squeeze()\n",
    "        x1 = x2 = x\n",
    "        \n",
    "        for i, l in enumerate(self.fclist1):\n",
    "            x1 = torch.nn.functional.relu(l(x1))\n",
    "        x1     = self.fc1(x1).squeeze()\n",
    "        \n",
    "        for i, l in enumerate(self.fclist2):\n",
    "            x2 = torch.nn.functional.relu(l(x2))\n",
    "        x2     = self.fc2(x2).squeeze()\n",
    "        \n",
    "        capf   = torch.log(\n",
    "            ((1 + torch.mul(x1, p))**2 + (torch.mul(x2, p))**2))\n",
    "        \n",
    "        return torch.sigmoid(capf).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Loss(Module):\n",
    "    def __init__(self, size_average=None, reduce=None, reduction='mean'):\n",
    "        super(_Loss, self).__init__()\n",
    "        if size_average is not None or reduce is not None:\n",
    "            self.reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
    "        else:\n",
    "            self.reduction = reduction\n",
    "\n",
    "class WeightedMSELoss(_Loss):\n",
    "    __constants__ = ['reduction']\n",
    "\n",
    "    def __init__(self, size_average=None, reduce=None, reduction='mean'):\n",
    "        super(WeightedMSELoss, self).__init__(size_average, reduce, reduction)\n",
    "\n",
    "    def forward(self, input, target, weight):\n",
    "        return torch.mean(torch.mul(weight, (input - target)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data - Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train):\n",
    "    model.get_L1max(value_per_unit)\n",
    "    optimiser           = torch.optim.Adam(model.parameters(), lr)\n",
    "    criterion           = WeightedMSELoss()\n",
    "    \n",
    "    if use_gpu:\n",
    "        model                 = model.cuda()\n",
    "        X_train, y_train      = X_train.cuda(), y_train.cuda()\n",
    "\n",
    "    print(\" =================== BEGINNING TRAIN ==================== \")\n",
    "    beginning = start = time.time()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        output          = model(X_train)\n",
    "        loss            = criterion(output, y_train, X_train[:, -1:])\n",
    "\n",
    "        if (e+1) % verbose_prd  == 0:\n",
    "            start       = report_ETA(beginning, start, epochs, e+1, loss)\n",
    "            if save_history:\n",
    "                generictitle = '%s, %s, %s, N=%d, epochs=%d'%(outputheader, pm, str(n_neurons), N, e+1)\n",
    "                torch.save({'state_dict': model.state_dict()}, outputfolder + \n",
    "                                generictitle + '.pth')\n",
    "                modelparams = [w.detach().tolist() for w in model.parameters()]\n",
    "                np.savetxt(outputfolder + generictitle + '.csv', modelparams, '%s')            \n",
    "            \n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        model.clip_L1()\n",
    "        \n",
    "    print(\" ===================   END OF TRAIN   =================== \")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding ratio of PT/pt instead of PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(pm):\n",
    "    sm_file        = sm_filename_fn(pm)    \n",
    "    train_size     = len(bsm_coef)*N\n",
    "    \n",
    "    smdata         = h5py.File(datafolder + sm_filename + '_' + pm + '_nlo.h5', 'r') \n",
    "    nsm            = smdata['Number'][()]\n",
    "    \n",
    "    if isinstance(nsm, np.ndarray):\n",
    "        nsm = nsm[0]\n",
    "    \n",
    "    smdata         = torch.Tensor(smdata['Data'][()])\n",
    "    smdata         = take_ratio(smdata, -2, -3)\n",
    "    \n",
    "    smdata_lst     = [append_constant(smdata[i*N:(i+1)*N, :], bsm_coef[i]\n",
    "                                 ) for i in range(len(bsm_coef))]    # wilson coefficient G for SM\n",
    "    smdata_lst.append(\n",
    "        append_constant(smdata[train_size:, :], bsm_test))           # wilson coefficient G for SM (test)\n",
    "    smdata         = torch.cat(smdata_lst, 0)\n",
    "    \n",
    "    #################################################################################################\n",
    "    #### be careful! as this changes the position of the column of weight and wilson coefficient ####\n",
    "    nlo_w_mean_sm  = smdata[:, -2].mean()\n",
    "    smdata         = multiply_by_constant(smdata, \n",
    "                          1./nlo_w_mean_sm, -2, inplace=False)     # sigma_g/sigma_0 = 1 for SM, NLO\n",
    "    #################################################################################################\n",
    "    \n",
    "    smdata         = append_constant(smdata, 0)                      # training target y = 0 for SM\n",
    "    \n",
    "    nbsm_dic       = {c: (h5py.File(datafolder + bsm_filename_fn(bsm_op, c, pm), \n",
    "                                    'r')['Number'][()]) for c in bsm_coef}\n",
    "        \n",
    "    if isinstance(nbsm_dic[bsm_coef[0]], np.ndarray):\n",
    "        nbsm_dic = {c: torch.as_tensor(nbsm_dic[c][0]) for c in bsm_coef}\n",
    "    else:\n",
    "        nbsm_dic = {c: torch.as_tensor(nbsm_dic[c]) for c in bsm_coef}\n",
    "    \n",
    "    \n",
    "    #nbsm_dic       = {c: (torch.as_tensor(h5py.File(datafolder + bsm_filename(bsm_op, c)\\\n",
    "    #                + '_' + pm + '_nlo.h5', 'r')['Number'][()])) for c in bsm_coef}\n",
    "    \n",
    "    bsmdata_lst    = [torch.Tensor(h5py.File(datafolder + bsm_filename_fn(bsm_op, c, pm),\n",
    "                                             'r')['Data'][()])[:N, :] for c in bsm_coef]\n",
    "    \n",
    "    print('============ nsm: %s ============'%(str(nsm)))\n",
    "    for i in range(len(bsm_coef)):\n",
    "        print('========== nbsm %i: %s =========='%(i, str(nbsm_dic[bsm_coef[i]])))\n",
    "    \n",
    "    bsmdata_lst    = [take_ratio(bsmdata_lst[i], -2, -3) for i in range(len(bsmdata_lst))]\n",
    "\n",
    "    bsmdata_lst    = [append_constant(bsmdata_lst[i], bsm_coef[i]\n",
    "                                 ) for i in range(len(bsmdata_lst))] # wilson coefficient G for BSM\n",
    "    \n",
    "    nlo_w_mean_bsm = [bsmdata_lst[i][:, -2].mean() for i in range(len(bsmdata_lst))]   \n",
    "    \n",
    "    bsmdata_lst    = [multiply_by_constant(bsmdata_lst[i], (nbsm_dic[bsm_coef[i]]/nsm)/nlo_w_mean_bsm[i],\n",
    "                            -2, inplace=False) for i in range(len(bsmdata_lst))] # sigma_g/sigma_0 = nbsm/nsm for BSM, NLO\n",
    "    \n",
    "    bsmdata        = torch.cat(bsmdata_lst, 0)\n",
    "    bsmdata        = append_constant(bsmdata, 1)                     # training target y = 1 for BSM\n",
    "\n",
    "    traindata      = torch.cat((smdata[:train_size, :], bsmdata), 0)\n",
    "    traindata      = traindata[torch.randperm(traindata.size(0))]\n",
    "\n",
    "    if convert_ang:\n",
    "        traindata  = convert_angles_in_data(traindata, angle_pos)\n",
    "\n",
    "    X_train        = traindata[:, :-1] \n",
    "    y_train        = traindata[:, -1].reshape(-1, 1)\n",
    "    \n",
    "    title = 'Scaling (input dim %d, ratio), %s, %s'%(input_dim, pm, bsm_coef)\n",
    "\n",
    "    if os.path.isfile(scalingfolder + title + '.csv'):\n",
    "        print('Reading scaling from: \\n%s...'%(scalingfolder + title + '.csv'))\n",
    "        \n",
    "        scalingPars  = np.loadtxt(open(scalingfolder + title + '.csv', 'r'))\n",
    "        X_train_mean = torch.from_numpy(scalingPars[:input_dim+1]).type(torch.FloatTensor)\n",
    "        X_train_std  = torch.from_numpy(scalingPars[input_dim+1:]).type(torch.FloatTensor)\n",
    "    else:\n",
    "        X_train_mean           = X_train[:,  :input_dim+1].mean(0)\n",
    "        X_train_std            = X_train[:,  :input_dim+1].std(0)\n",
    "    \n",
    "    # normalisation\n",
    "    X_train[:, :input_dim+1] = (X_train[:, :input_dim+1] - X_train_mean)/X_train_std\n",
    "    \n",
    "    #################################################################################################\n",
    "    \n",
    "    bsmdata_test   = h5py.File(datafolder + bsm_filename(bsm_op, bsm_test) +\\\n",
    "                            '_' + pm +'_nlo.h5', 'r')\n",
    "    nbsm_dic[bsm_test] = (bsmdata_test['Number'][()])\n",
    "    \n",
    "    if isinstance(nbsm_dic[bsm_test], np.ndarray):\n",
    "        nbsm_dic[bsm_test] = torch.as_tensor(nbsm_dic[bsm_test][0])\n",
    "    else:\n",
    "        nbsm_dic[bsm_test] = torch.as_tensor(nbsm_dic[bsm_test])\n",
    "    \n",
    "    bsmdata_test   = torch.Tensor(bsmdata_test['Data'][()])\n",
    "    bsmdata_test   = take_ratio(bsmdata_test, -2, -3)\n",
    "    \n",
    "    bsmdata_test   = append_constant(bsmdata_test, bsm_test)         # wilson coefficient G for BSM\n",
    "    \n",
    "    #################################################################################################\n",
    "    #### be careful! as this changes the position of the column of weight and wilson coefficient ####\n",
    "    nlo_w_mean_bsm_test = bsmdata_test[:, -2].mean() \n",
    "    \n",
    "    bsmdata_test   = multiply_by_constant(bsmdata_test, \n",
    "        (nbsm_dic[bsm_test]/nsm)/nlo_w_mean_bsm_test, -2, inplace=False)# sigma_g/sigma_0 = nbsm/nsm for BSM\n",
    "    #################################################################################################\n",
    "    \n",
    "    bsmdata_test   = append_constant(bsmdata_test, 1)                # testing target y = 1 for BSM\n",
    "\n",
    "    if bsm_test in bsm_coef:\n",
    "        bsmdata_test  = bsmdata_test[N:, :]\n",
    "\n",
    "    smdata_test    = smdata[train_size:,  :]\n",
    "    \n",
    "    #testdata       = torch.cat((smdata[train_size:, :], bsmdata_test), 0)\n",
    "    #testdata       = testdata[torch.randperm(testdata.size(0))]\n",
    "\n",
    "    if convert_ang:\n",
    "        smdata_test   = convert_angles_in_data(smdata_test,  angle_pos)\n",
    "        bsmdata_test  = convert_angles_in_data(bsmdata_test, angle_pos)\n",
    "    \n",
    "    X_test_sm      = smdata_test[:,  :-1] \n",
    "    X_test_bsm     = bsmdata_test[:, :-1]\n",
    "    \n",
    "    # normalisation\n",
    "    X_test_sm[:,  :input_dim+1]  = (X_test_sm[:, :input_dim+1] - X_train_mean)/X_train_std\n",
    "    X_test_bsm[:, :input_dim+1] = (X_test_bsm[:, :input_dim+1] - X_train_mean)/X_train_std\n",
    "    \n",
    "    if not os.path.isfile(scalingfolder + title + '.csv'):\n",
    "        print('Saving scaling parameters at: \\n%s...'%(scalingfolder + title + '.csv'))\n",
    "        \n",
    "        np.savetxt(outputfolder + title + '.csv', torch.cat(\n",
    "        [X_train_mean.reshape(-1, 1), X_train_std.reshape(-1, 1)], 0).numpy())\n",
    "        \n",
    "    return X_train, y_train, X_test_sm, X_test_bsm, nsm, nbsm_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafolder    = '/home/chen/Documents/DibosonProcessData_NLO_NewBias/'\n",
    "scalingfolder = '/home/chen/Documents/OutputQuadratic_NLO_NewBias_alt/'\n",
    "outputfolder  = '/home/chen/Documents/OutputQuadratic_NLO_NewBias_alt/'\n",
    "outputheader  = 'Newbias'\n",
    "\n",
    "sm_filename_fn   = lambda pm: 'sm_%s_nlo.h5'%(pm)\n",
    "bsm_filename_fn  = lambda g, c, pm: '%s%s_%s_nlo.h5'%(g, c, pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_pos     = [3, 5]\n",
    "convert_ang   = True\n",
    "\n",
    "use_gpu       = True\n",
    "input_dim     = 8 if not convert_ang else 10\n",
    "n_neurons     = [input_dim, 32, 32, 32]\n",
    "#N             = int(18e4)\n",
    "N             = int(2e5)\n",
    "lr            = 1e-3\n",
    "epochs        = 10000\n",
    "verbose_prd   = 1000\n",
    "n_meas        = 4000\n",
    "value_per_unit= 1.0\n",
    "save_history  = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputheader  = 'NewBias-GW-vanilla'\n",
    "bsm_op        = 'GW'\n",
    "bsm_coef      = ['-1e-7', '-5e-8', '-2e-8', '2e-8', '5e-8', '1e-7']\n",
    "bsm_test      = '2e-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ nsm: 3808.005957692273 ============\n",
      "========== nbsm 0: tensor(4868.6577) ==========\n",
      "========== nbsm 1: tensor(4117.4551) ==========\n",
      "========== nbsm 2: tensor(3873.3442) ==========\n",
      "========== nbsm 3: tensor(3840.9983) ==========\n",
      "========== nbsm 4: tensor(4003.5100) ==========\n",
      "========== nbsm 5: tensor(4674.4160) ==========\n",
      "Reading scaling from: \n",
      "/home/chen/Documents/OutputQuadratic_NLO_NewBias_alt/Scaling (input dim 10, ratio), plus, ['-1e-7', '-5e-8', '-2e-8', '2e-8', '5e-8', '1e-7'].csv...\n",
      " =================== BEGINNING TRAIN ==================== \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-55e2c0d44373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_sm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_bsm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbsm_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_plus\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0mQuadraticNetReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_plus\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_plus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpm\u001b[0m                    \u001b[0;34m=\u001b[0m \u001b[0;34m'minus'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-7862a5f4a231>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, X_train, y_train)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_L1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" ===================   END OF TRAIN   =================== \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-fd129a27c35a>\u001b[0m in \u001b[0;36mclip_L1\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m                         \u001b[0mL1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                         mm.weight.masked_scatter_(L1>designated_L1,\n\u001b[0;32m---> 62\u001b[0;31m                                             mm.weight/L1*designated_L1)\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pm                  = 'plus'\n",
    "X_train, y_train, X_test_sm, X_test_bsm, nsm, nbsm_dic = read_data(pm)\n",
    "model_plus          = QuadraticNetReLU()\n",
    "model_plus          = train(model_plus, X_train, y_train)\n",
    "\n",
    "pm                    = 'minus'\n",
    "X_train, y_train, X_test_sm, X_test_bsm, nsm, nbsm_dic = read_data(pm)\n",
    "model_minus           = QuadraticNetReLU()\n",
    "model_minus           = train(model_minus, X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
