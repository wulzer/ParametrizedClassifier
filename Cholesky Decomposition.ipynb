{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OurTrainingTools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurCDModel(nn.Module):\n",
    "### Defines the  model with parametrized discriminant. Only quadratic dependence on a single parameter is implemented.\n",
    "### Input is the architecture (list of integers, the last one being equal to 1) and the activation type ('ReLU' or 'Sigmoid')\n",
    "    def __init__(self, NumberOfParameters, AR = [1, 3, 3, 1] , AF = 'ReLU'):               \n",
    "        super(OurCDModel, self).__init__() \n",
    "        ValidActivationFunctions = {'ReLU': torch.relu, 'Sigmoid': torch.sigmoid}\n",
    "        try:\n",
    "            self.ActivationFunction = ValidActivationFunctions[AF]\n",
    "        except KeyError:\n",
    "            print('The activation function specified is not valid. Allowed activations are %s.'\n",
    "                 %str(list(ValidActivationFunctions.keys())))\n",
    "            print('Will use ReLU.')\n",
    "            self.ActivationFunction = torch.relu            \n",
    "        if type(AR) == list:\n",
    "            if( ( all(isinstance(n, int) for n in AR)) and ( AR[-1] == 1) ):\n",
    "                self.Architecture = AR\n",
    "            else:\n",
    "                print('Architecture should be a list of integers, the last one should be 1.')\n",
    "                raise ValueError             \n",
    "        else:\n",
    "            print('Architecture should be a list !')\n",
    "            raise ValueError\n",
    "        self.NumberOfParameters = NumberOfParameters\n",
    "\n",
    "### Define Layers\n",
    "        self.NumberOfNetworks = ((2+NumberOfParameters)*(1+NumberOfParameters)/2)-1\n",
    "        LinearLayers = [([nn.Linear(self.Architecture[i], self.Architecture[i+1]) \\\n",
    "                                  for i in range(len(self.Architecture)-1)])\\\n",
    "                        for n in range(self.NumberOfNetworks)]\n",
    "        LinearLayers = [Layer for SubLayerList in LinearLayers for Layer in SubLayerList]\n",
    "        self.LinearLayers = nn.ModuleList(LinearLayers)\n",
    "        \n",
    "    def Forward(self, Data, Parameters):\n",
    "### Forward Function. Performs Preprocessing, returns F = rho/(1+rho) in [0,1], where rho is quadratically parametrized.\n",
    "        # Checking that data has the right input dimension\n",
    "        InputDimension = self.Architecture[0]\n",
    "        if Data.size(1) != InputDimension:\n",
    "            print('Dimensions of the data and the network input mismatch: data: %d, model: %d'\n",
    "                  %(Data.size(1), InputDimension))\n",
    "            raise ValueError\n",
    "\n",
    "        # Checking that preprocess has been initialised\n",
    "        if not hasattr(self, 'Shift'):\n",
    "            print('Please initialize preprocess parameters!')\n",
    "            raise ValueError\n",
    "        with torch.no_grad(): \n",
    "            Data, Parameters = self.Preprocess(Data, Parameters)  \n",
    "        \n",
    "        NumberOfLayers, NumberOfEvents = len(self.Architecture)-1, Data.size(0)\n",
    "        EntryIterator, NetworkIterator = 0, -1\n",
    "        MatrixLT = torch.zeros([NumberOfEvents, (self.NumberOfParameters+1)**2], dtype=Data.dtype)\n",
    "        \n",
    "        if Data.is_cuda:\n",
    "            MatrixLT = OurCudaTensor(MatrixLT)\n",
    "        \n",
    "        for i in range(self.NumberOfParameters+1):\n",
    "            EntryIterator += i\n",
    "            for j in range(self.NumberOfParameters+1-i):\n",
    "                if NetworkIterator == -1:\n",
    "                    MatrixLT[:, EntryIterator] = torch.ones(NumberOfEvents)\n",
    "                    print('Entry: %d, Layer: ones'%(EntryIterator))\n",
    "                else:\n",
    "                    x = Data\n",
    "                    for Layer in self.LinearLayers[NumberOfLayers*NetworkIterator:\\\n",
    "                                                  NumberOfLayers*(NetworkIterator+1)-1]:\n",
    "                        x = self.ActivationFunction(Layer(x))\n",
    "                    x = self.LinearLayers[NumberOfLayers*(NetworkIterator+1)-1](x).squeeze()\n",
    "                    MatrixLT[:, EntryIterator] = x\n",
    "                    print('Entry: %d, Layer: %d'%(EntryIterator, NetworkIterator))\n",
    "                EntryIterator += 1\n",
    "                NetworkIterator += 1\n",
    "        #print('MatrixLT: '+str(MatrixLT.is_cuda))\n",
    "        #print('Parameters: '+str(Parameters.is_cuda))\n",
    "\n",
    "        MatrixLT = MatrixLT.reshape([-1, self.NumberOfParameters+1, self.NumberOfParameters+1])\n",
    "        MatrixLTP = MatrixLT.matmul(Parameters.reshape([NumberOfEvents, self.NumberOfParameters+1, 1]))\n",
    "        rho = MatrixLTP.permute([0, 2, 1]).matmul(MatrixLTP).squeeze()\n",
    "        \n",
    "        return (rho.div(1.+rho)).view(-1, 1)\n",
    "    \n",
    "    def GetL1Bound(self, L1perUnit):\n",
    "        self.L1perUnit = L1perUnit\n",
    "    \n",
    "    def ClipL1Norm(self):\n",
    "### Clip the weights      \n",
    "        def ClipL1NormLayer(DesignatedL1Max, Layer, Counter):\n",
    "            if Counter == 1:\n",
    "                ### this avoids clipping the first layer\n",
    "                return\n",
    "            L1 = Layer.weight.abs().sum()\n",
    "            Layer.weight.masked_scatter_(L1 > DesignatedL1Max, \n",
    "                                        Layer.weight*(DesignatedL1Max/L1))\n",
    "            return\n",
    "        \n",
    "        Counter = 0\n",
    "        for m in self.children():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                Counter += 1\n",
    "                with torch.no_grad():\n",
    "                    DesignatedL1Max = m.weight.size(0)*m.weight.size(1)*self.L1perUnit\n",
    "                    ClipL1NormLayer(DesignatedL1Max, m, Counter)\n",
    "            else:\n",
    "                for mm in m:\n",
    "                    Counter +=1\n",
    "                    with torch.no_grad():\n",
    "                        DesignatedL1Max = mm.weight.size(0)*m.weight.size(1)*self.L1perUnit\n",
    "                        ClipL1NormLayer(DesignatedL1Max, mm, Counter)\n",
    "        return \n",
    "    \n",
    "    def DistributionRatio(self, points):\n",
    "### This is rho. I.e., after training, the estimator of the distribution ratio.\n",
    "        with torch.no_grad():\n",
    "            F = self(points)\n",
    "        return F/(1-F)\n",
    "\n",
    "    def InitPreprocess(self, Data, Parameters):\n",
    "### This can be run only ONCE to initialize the preprocess (shift and scaling) parameters\n",
    "### Takes as input the training Data and the training Parameters as Torch tensors.\n",
    "        if not hasattr(self, 'Scaling'):\n",
    "            print('Initializing Preprocesses Variables')\n",
    "            self.Scaling = Data.std(0)\n",
    "            self.Shift = Data.mean(0)\n",
    "            self.ParameterScaling = Parameters.std(0)  \n",
    "        else: print('Preprocess can be initialized only once. Parameters unchanged.')\n",
    "            \n",
    "    def Preprocess(self, Data, Parameters):\n",
    "### Returns scaled/shifted data and parameters\n",
    "### Takes as input Data and Parameters as Torch tensors.\n",
    "        if  not hasattr(self, 'Scaling'): print('Preprocess parameters are not initialized.')\n",
    "        Data = (Data - self.Shift)/self.Scaling\n",
    "        Parameters = Parameters/self.ParameterScaling\n",
    "        Ones = torch.ones([Parameters.size(0),1], dtype=Parameters.dtype)\n",
    "        if Parameters.is_cuda:\n",
    "            Ones = Ones.cuda()\n",
    "        Parameters = torch.cat([Ones, Parameters.reshape(-1,1)], dim=1)\n",
    "        return Data, Parameters\n",
    "    \n",
    "    def Save(self, Name, Folder, csvFormat=False):\n",
    "### Saves the model in Folder/Name\n",
    "        FileName = Folder + Name + '.pth'\n",
    "        torch.save({'StateDict': self.state_dict(), \n",
    "                   'Scaling': self.Scaling,\n",
    "                   'Shift': self.Shift,\n",
    "                   'ParameterScaling': self.ParameterScaling}, \n",
    "                   FileName)\n",
    "        print('Model successfully saved.')\n",
    "        print('Path: %s'%str(FileName))\n",
    "        \n",
    "        if csvFormat:\n",
    "            modelparams = [w.detach().tolist() for w in self.parameters()]\n",
    "            np.savetxt(Folder + Name + ' (StateDict).csv', modelparams, '%s')\n",
    "            statistics = [self.Shift.detach().tolist(), self.Scaling.detach().tolist(),\n",
    "                         self.ParameterScaling.detach().tolist()]\n",
    "            np.savetxt(Folder + Name + ' (Statistics).csv', statistics, '%s')\n",
    "    \n",
    "    def Load(self, Name, Folder):\n",
    "### Loads the model from Folder/Name\n",
    "        FileName = Folder + Name + '.pth'\n",
    "        try:\n",
    "            IncompatibleKeys = self.load_state_dict(torch.load(FileName)['StateDict'])\n",
    "        except KeyError:\n",
    "            print('No state dictionary saved. Loading model failed.')\n",
    "            return \n",
    "        \n",
    "        if list(IncompatibleKeys)[0]:\n",
    "            print('Missing Keys: %s'%str(list(IncompatibleKeys)[0]))\n",
    "            print('Loading model failed. ')\n",
    "            return \n",
    "        \n",
    "        if list(IncompatibleKeys)[1]:\n",
    "            print('Unexpected Keys: %s'%str(list(IncompatibleKeys)[0]))\n",
    "            print('Loading model failed. ')\n",
    "            return \n",
    "        \n",
    "        self.Scaling = torch.load(FileName)['Scaling']\n",
    "        self.Shift = torch.load(FileName)['Shift']\n",
    "        self.ParameterScaling = torch.load(FileName)['ParameterScaling']\n",
    "        \n",
    "        print('Model successfully loaded.')\n",
    "        print('Path: %s'%str(FileName))\n",
    "        \n",
    "    def Report(self): ### is it possibe to check if the model is in double?\n",
    "        print('\\nModel Report:')\n",
    "        print('Preprocess Initialized: ' + str(hasattr(self, 'Shift')))\n",
    "        print('Architecture: ' + str(self.Architecture))\n",
    "        print('Loss Function: ' + 'Quadratic')\n",
    "        print('Activation: ' + str(self.ActivationFunction))\n",
    "        \n",
    "    def cuda(self):\n",
    "        nn.Module.cuda(self)\n",
    "        self.Shift = self.Shift.cuda()\n",
    "        self.Scaling = self.Scaling.cuda()\n",
    "        self.ParameterScaling = self.ParameterScaling.cuda()\n",
    "        \n",
    "    def cpu(self):\n",
    "        self.Shift = self.Shift.cpu()\n",
    "        self.Scaling = self.Scaling.cpu()\n",
    "        self.ParameterScaling = self.ParameterScaling.cpu()\n",
    "        return nn.Module.cpu(self)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data Files for Process: W+Z, with new physics Parameters: ['GW[TeV**-2]']\n",
      "\n",
      "Reading file .../data3/Training/TrainingData/GWData/ChPgw2.h5\n",
      "##### File Info:\n",
      "GW = 0.02[Tev**-2] data, showering on. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ, PtWZ, PtW, mz2, mw2}, weight}. Converted from /data3/ConvertMGevents/Showered_Events_for_Training/ChPLong290gw2.dat\n",
      " File with charge = 1\n",
      "#####\n",
      "\n",
      "Reading file .../data3/Training/TrainingData/GWData/ChPgw5.h5\n",
      "##### File Info:\n",
      "GW = 0.05[Tev**-2] data, showering on. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ, PtWZ, PtW, mz2, mw2}, weight}. Converted from /data3/ConvertMGevents/Showered_Events_for_Training/ChPLong290gw5.dat\n",
      " File with charge = 1\n",
      "#####\n",
      "\n",
      "Reading file .../data3/Training/TrainingData/GWData/ChPgw10.h5\n",
      "##### File Info:\n",
      "GW = 0.1[Tev**-2] data, showering on. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ, PtWZ, PtW, mz2, mw2}, weight}. Converted from /data3/ConvertMGevents/Showered_Events_for_Training/ChPLong290gw10.dat\n",
      " File with charge = 1\n",
      "#####\n",
      "\n",
      "Reading file .../data3/Training/TrainingData/GWData/ChPgwm2.h5\n",
      "##### File Info:\n",
      "GW = -0.02[Tev**-2] data, showering on. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ, PtWZ, PtW, mz2, mw2}, weight}. Converted from /data3/ConvertMGevents/Showered_Events_for_Training/ChPLong290gwm2.dat\n",
      " File with charge = 1\n",
      "#####\n",
      "\n",
      "Reading file .../data3/Training/TrainingData/GWData/ChPgwm5.h5\n",
      "##### File Info:\n",
      "GW = -0.05[Tev**-2] data, showering on. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ, PtWZ, PtW, mz2, mw2}, weight}. Converted from /data3/ConvertMGevents/Showered_Events_for_Training/ChPLong290gwm5.dat\n",
      " File with charge = 1\n",
      "#####\n",
      "\n",
      "Reading file .../data3/Training/TrainingData/GWData/ChPgwm10.h5\n",
      "##### File Info:\n",
      "GW = -0.1[Tev**-2] data, showering on. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ, PtWZ, PtW, mz2, mw2}, weight}. Converted from /data3/ConvertMGevents/Showered_Events_for_Training/ChPLong290gwm10.dat\n",
      " File with charge = 1\n",
      "#####\n",
      "\n",
      "Reading file .../data3/Training/TrainingData/SMData/ChPsm.h5\n",
      "##### File Info:\n",
      "SM = 0.[Tev**-2] data, showering on. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ, PtWZ, PtW, mz2, mw2}, weight}. Converted from /data3/ConvertMGevents/Showered_Events_for_Training/ChPLong290sm.dat\n",
      " File with charge = 1\n",
      "#####\n",
      "\n",
      "Reading file .../data3/Training/TrainingData/SMData/ChPsm2.h5\n",
      "##### File Info:\n",
      "SM = 0.[Tev**-2] data, showering on. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ, PtWZ, PtW, mz2, mw2}, weight}. Converted from /data3/ConvertMGevents/Showered_Events_for_Training/ChPLong290sm2.dat\n",
      " File with charge = 1\n",
      "#####\n",
      "\n",
      "Reading file .../data3/Training/TrainingData/SMData/ChPsm3.h5\n",
      "##### File Info:\n",
      "SM = 0.[Tev**-2] data, showering on. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ, PtWZ, PtW, mz2, mw2}, weight}. Converted from /data3/ConvertMGevents/Showered_Events_for_Training/ChPLong290sm3.dat\n",
      " File with charge = 1\n",
      "#####\n",
      "\n",
      "Loaded SM Files:\n",
      "  ['GW[TeV**-2]']    #Data    XS[pb](avg.w)\n",
      "-----------------  -------  ---------------\n",
      "                0   566980      0.000322493\n",
      "                0   566980      0.000322493\n",
      "                0   566980      0.000322493\n",
      "\n",
      "Loaded BSM Files:\n",
      "  ['GW[TeV**-2]']    #Data    XS[pb](avg.w)\n",
      "-----------------  -------  ---------------\n",
      "             0.02   571169      0.000324063\n",
      "             0.05   597532      0.000340505\n",
      "             0.1    683831      0.000404072\n",
      "            -0.02   573538      0.000329051\n",
      "            -0.05   602355      0.000351064\n",
      "            -0.1    693132      0.000423358\n",
      "\n",
      "Paired BSM/SM Datasets:\n",
      "\n",
      "  ['GW[TeV**-2]']    #Ev.BSM    #Ev.SM    Check\n",
      "-----------------  ---------  --------  -------\n",
      "             0.02     200000    200000   200045\n",
      "             0.05     200000    200000   199955\n",
      "             0.1      200000    200000   200045\n",
      "            -0.02     200000    200000   199955\n",
      "            -0.05     200000    200000   200045\n",
      "            -0.1      200000    200000   199955\n",
      "Initializing Preprocesses Variables\n",
      "Training epoch 10 (took 17.29 sec, time left 1 day, 19:39:55.906770 sec) loss 0.01257529\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/ParametrizedClassifier/TrainedModels/ChPgw2,5,10, (200k, Cholesky Decomposition), 10 epoch.pth\n",
      "Training epoch 100 (took 151.16 sec, time left 1 day, 22:17:23.250990 sec) loss 0.00968958\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/ParametrizedClassifier/TrainedModels/ChPgw2,5,10, (200k, Cholesky Decomposition), 100 epoch.pth\n",
      "Training epoch 500 (took 563.46 sec, time left 1 day, 16:22:49.327863 sec) loss 0.00958517\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/ParametrizedClassifier/TrainedModels/ChPgw2,5,10, (200k, Cholesky Decomposition), 500 epoch.pth\n",
      "Training epoch 1000 (took 780.20 sec, time left 1 day, 17:32:36.723434 sec) loss 0.00951232\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/ParametrizedClassifier/TrainedModels/ChPgw2,5,10, (200k, Cholesky Decomposition), 1000 epoch.pth\n"
     ]
    }
   ],
   "source": [
    "td = OurTrainingData(['/data3/Training/TrainingData/SMData/ChPsm.h5',\n",
    "                     '/data3/Training/TrainingData/SMData/ChPsm2.h5',\n",
    "                     '/data3/Training/TrainingData/SMData/ChPsm3.h5',],\n",
    "                    ['/data3/Training/TrainingData/GWData/ChPgw2.h5',\n",
    "                     '/data3/Training/TrainingData/GWData/ChPgw5.h5',\n",
    "                     '/data3/Training/TrainingData/GWData/ChPgw10.h5',\n",
    "                     '/data3/Training/TrainingData/GWData/ChPgwm2.h5',\n",
    "                     '/data3/Training/TrainingData/GWData/ChPgwm5.h5',\n",
    "                     '/data3/Training/TrainingData/GWData/ChPgwm10.h5',],\n",
    "                     process = 'W+Z', parameters =['GW[TeV**-2]'], \n",
    "                     SMNLimits=int(4e2),\n",
    "                     BSMNLimits=int(2e2))\n",
    "\n",
    "Data, ParVal, Labels, Weights = td.Data[:, :8], td.ParVal, td.Labels, td.Weights\n",
    "MD = OurCDModel(NumberOfParameters=1, AR=[8,32,32,32,1])\n",
    "MD.double()\n",
    "MD.InitPreprocess(Data, ParVal)\n",
    "OT = OurTrainer(NumEpochs = int(1e4))\n",
    "OT.SetSaveAfterEpochs([10,100,500]+list(range(1000, 11000, 1000)))\n",
    "\n",
    "OT.Train(MD, Data = Data, Parameters = ParVal, Labels=Labels, Weights= Weights, bs = 100000,\n",
    "        Name = 'ChPgw2,5,10, (200k, Cholesky Decomposition), ', Folder = os.getcwd()+'/TrainedModels/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ParVal.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4800000])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ParVal.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 3]), torch.Size([100, 3, 3]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TempParameters.size(), (MatrixLT).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0776,  6.5665, -2.4170]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TempParameters.reshape([100, 1, 3]).matmul(MatrixLT.permute([0, 2, 1,]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.0000,  0.0000],\n",
       "        [-0.1251,  0.1466,  0.0000],\n",
       "        [ 0.0266,  0.1255, -0.0483]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MatrixLT[0].transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0776,  6.5665, -2.4170]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TempParameters[0].reshape([1, 3]).mm(MatrixLT[0].transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([53.2776, 41.2998, 39.7028, 60.2778, 37.5379, 50.8842, 40.7845, 41.8562,\n",
       "        41.7262, 57.7348, 62.5857, 64.4010, 40.5552, 36.0387, 41.7204, 44.0750,\n",
       "        41.8498, 55.1948, 57.6788, 33.6665, 38.0422, 78.1223, 34.6115, 55.5912,\n",
       "        52.7553, 29.8227, 45.6006, 56.3874, 29.8103, 46.8122, 55.6364, 38.1525,\n",
       "        47.8337, 40.9304, 45.7424, 25.9865, 48.1553, 60.4051, 72.2461, 59.1649,\n",
       "        30.3844, 46.5015, 40.5201, 40.7097, 55.5965, 41.4380, 54.1305, 43.2585,\n",
       "        46.9714, 39.4395, 62.3753, 74.8668, 54.3242, 34.5350, 47.0982, 48.9572,\n",
       "        41.5068, 39.7230, 44.4457, 53.8537, 73.3045, 55.2483, 40.0775, 39.9372,\n",
       "        49.8916, 48.3826, 34.7451, 23.2622, 54.1850, 40.4493, 48.1900, 54.7582,\n",
       "        49.6336, 40.2133, 49.5574, 36.5985, 46.7742, 59.3095, 41.3831, 42.0143,\n",
       "        45.2201, 38.8211, 69.8754, 37.5511, 47.5282, 41.5546, 43.2678, 59.7412,\n",
       "        25.9005, 38.3847, 63.8126, 33.0527, 52.6752, 63.7158, 58.9484, 36.8365,\n",
       "        49.9637, 33.6771, 63.9414, 52.8860], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MatrixLTP = MatrixLT.matmul(TempParameters.reshape([100, 3, 1]))\n",
    "MatrixLTP.permute([0, 2, 1]).matmul(MatrixLTP).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
